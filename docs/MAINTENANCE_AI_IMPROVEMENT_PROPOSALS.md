# ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AI æ”¹å–„ææ¡ˆ

å®Ÿè£…æ¸ˆã¿ã‚·ã‚¹ãƒ†ãƒ ã®åˆ†æã«åŸºã¥ãã€å„ªå…ˆåº¦ä»˜ãæ”¹å–„ææ¡ˆ

---

## ğŸ¯ å„ªå…ˆåº¦: é«˜ï¼ˆã™ãã«å®Ÿè£…æ¨å¥¨ï¼‰

### 1. æ¥­ç•Œæ¨™æº–ãƒ„ãƒ¼ãƒ«ã®æ®µéšçš„å°å…¥

#### A. Dependabotã®æ´»æ€§åŒ– âœ… **æ—¢ã«å­˜åœ¨**

**ç¾çŠ¶**: `.github/dependabot.yml` ã¯æ—¢ã«å®Ÿè£…æ¸ˆã¿
**æ¨å¥¨**: ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AIã¨ã®é€£æº

```python
# scripts/maintenance_ai.py ã«è¿½åŠ 
def check_dependabot_status(self):
    """Dependabot PRã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    self.log("Dependabot PRçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯", "INFO")
    
    try:
        # GitHub APIçµŒç”±ã§Dependabot PRã‚’å–å¾—
        result = subprocess.run(
            ["gh", "pr", "list", "--label", "dependencies", "--json", "number,title,updatedAt"],
            cwd=self.base_dir,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            prs = json.loads(result.stdout)
            
            # 30æ—¥ä»¥ä¸Šæ”¾ç½®ã•ã‚Œã¦ã„ã‚‹PRã‚’è­¦å‘Š
            for pr in prs:
                updated = datetime.fromisoformat(pr["updatedAt"].replace("Z", "+00:00"))
                days_old = (datetime.now(updated.tzinfo) - updated).days
                
                if days_old > 30:
                    self.add_issue(
                        "dependencies",
                        "WARNING",
                        f"Dependabot PR #{pr['number']} ãŒ{days_old}æ—¥é–“æ”¾ç½®ã•ã‚Œã¦ã„ã¾ã™",
                        auto_fix=False
                    )
                    
    except FileNotFoundError:
        self.add_issue(
            "dependencies",
            "INFO",
            "GitHub CLIãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆghï¼‰",
            auto_fix=False
        )
```

**åŠ¹æœ**:
- âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã®é…å»¶ã‚’æ¤œå‡º
- âœ… ä¾å­˜é–¢ä¿‚ã®æœ€æ–°æ€§ã‚’ç¶­æŒ

---

#### B. CodeQLçµ±åˆï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³ï¼‰

**å®Ÿè£…**: æ–°è¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¿½åŠ 

```yaml
# .github/workflows/codeql.yml
name: CodeQL Security Analysis

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 10 * * 1'  # æ¯é€±æœˆæ›œ 19:00 JST

jobs:
  analyze:
    name: CodeQL Analysis
    runs-on: ubuntu-latest
    permissions:
      security-events: write
      actions: read
      contents: read

    strategy:
      matrix:
        language: ['javascript', 'python']

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: ${{ matrix.language }}
          queries: security-and-quality

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:${{matrix.language}}"
```

**ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AIçµ±åˆ**:
```python
def check_security_alerts(self):
    """GitHub Security Alertsã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        # GitHub Security Advisoriesã‚’ç¢ºèª
        result = subprocess.run(
            ["gh", "api", "/repos/{owner}/{repo}/dependabot/alerts", "--jq", "length"],
            cwd=self.base_dir,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            alert_count = int(result.stdout.strip())
            
            if alert_count > 0:
                self.add_issue(
                    "security",
                    "CRITICAL",
                    f"{alert_count}ä»¶ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ©ãƒ¼ãƒˆãŒæœªè§£æ±ºã§ã™",
                    auto_fix=False
                )
    except Exception as e:
        self.log(f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}", "WARNING")
```

**åŠ¹æœ**:
- âœ… SQL injection, XSS ãªã©ã®è„†å¼±æ€§è‡ªå‹•æ¤œå‡º
- âœ… ç„¡æ–™ï¼ˆGitHubãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒªãƒã‚¸ãƒˆãƒªï¼‰

---

### 2. å“è³ªç¥çµŒç³»çµ±ã¨ã®æ·±ã„çµ±åˆ

**ç¾çŠ¶**: `maintenance_ai.py` ã¯ `quality_nervous_system.py` ã‚’å®Ÿè¡Œã™ã‚‹ã®ã¿
**å•é¡Œ**: è©³ç´°ãªå•é¡Œå†…å®¹ã‚’å–å¾—ã§ããªã„

#### æ”¹å–„æ¡ˆA: JSONå‡ºåŠ›å½¢å¼ã®çµ±ä¸€

```python
# scripts/quality_nervous_system.py ã«è¿½åŠ 
def export_json_report(issues: List[Dict], output_path: str = "quality_nervous_system_report.json"):
    """JSONå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "total_issues": len(issues),
        "critical_issues": len([i for i in issues if i["severity"] == "CRITICAL"]),
        "issues": issues
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    return output_path

# ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®æœ€å¾Œã§å‘¼ã³å‡ºã—
if __name__ == "__main__":
    # ... æ—¢å­˜ã®å‡¦ç† ...
    
    # JSONå‡ºåŠ›
    if os.environ.get("EXPORT_JSON") == "1":
        export_json_report(all_issues)
```

**ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AIå´ã®æ”¹å–„**:
```python
def check_data_quality(self):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯"""
    self.log("ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹", "INFO")
    
    try:
        # JSONå‡ºåŠ›ã‚’æœ‰åŠ¹åŒ–
        env = os.environ.copy()
        env["EXPORT_JSON"] = "1"
        
        result = subprocess.run(
            ["python3", "scripts/quality_nervous_system.py"],
            cwd=self.base_dir,
            capture_output=True,
            text=True,
            timeout=300,
            env=env
        )
        
        # JSON ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        report_path = self.base_dir / "quality_nervous_system_report.json"
        if report_path.exists():
            with open(report_path, "r", encoding="utf-8") as f:
                quality_report = json.load(f)
            
            # è©³ç´°ãªå•é¡Œã‚’å€‹åˆ¥ã«è¨˜éŒ²
            for issue in quality_report.get("issues", []):
                self.add_issue(
                    "data_quality",
                    issue.get("severity", "WARNING"),
                    f"{issue['file_path']}: {issue['description']}",
                    file_path=issue.get("file_path"),
                    auto_fix=False
                )
                
    except Exception as e:
        self.log(f"å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
```

**åŠ¹æœ**:
- âœ… å•é¡Œã®è©³ç´°ã‚’ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AIãƒ¬ãƒãƒ¼ãƒˆã«çµ±åˆ
- âœ… ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ»å•é¡Œåˆ¥ã®è¿½è·¡ãŒå¯èƒ½

---

### 3. è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ã®æ‹¡å¼µ

**ç¾çŠ¶**: `auto_fix` ãƒ•ãƒ©ã‚°ã¯ã‚ã‚‹ãŒã€å®Ÿè£…ã¯ `npm audit fix` ã®ã¿

#### å®Ÿè£…ã™ã¹ãè‡ªå‹•ä¿®æ­£

##### A. ã‚³ãƒ¼ãƒ‰å“è³ªã®è‡ªå‹•ä¿®æ­£

```python
def apply_code_quality_fixes(self):
    """ã‚³ãƒ¼ãƒ‰å“è³ªã®è‡ªå‹•ä¿®æ­£"""
    fixes = []
    
    # 1. ESLint è‡ªå‹•ä¿®æ­£
    fixes.append({
        "type": "eslint",
        "command": "npm run lint:fix",
        "description": "ESLintè‡ªå‹•ä¿®æ­£"
    })
    
    # 2. Prettier ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    fixes.append({
        "type": "prettier",
        "command": "npx prettier --write 'src/**/*.{ts,tsx,css}'",
        "description": "ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"
    })
    
    # 3. æœªä½¿ç”¨importå‰Šé™¤
    fixes.append({
        "type": "unused-imports",
        "command": "npx ts-prune --ignore 'src/vite-env.d.ts|*.test.ts'",
        "description": "æœªä½¿ç”¨importãƒã‚§ãƒƒã‚¯"
    })
    
    return fixes
```

##### B. ãƒ‡ãƒ¼ã‚¿å“è³ªã®è‡ªå‹•ä¿®æ­£

```python
def apply_data_quality_fixes(self):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã®è‡ªå‹•ä¿®æ­£"""
    fixes = []
    
    # èªå½™å¤šæ§˜æ€§ä¸è¶³ã®è‡ªå‹•ä¿®æ­£
    # ï¼ˆæ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ´»ç”¨ï¼‰
    if self._has_vocabulary_diversity_issue():
        fixes.append({
            "type": "vocabulary",
            "command": "python3 scripts/improve_vocabulary_diversity.py --auto-fix",
            "description": "èªå½™å¤šæ§˜æ€§ã®æ”¹å–„"
        })
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸å“è³ªã®è‡ªå‹•ä¿®æ­£
    if self._has_passage_quality_issue():
        fixes.append({
            "type": "passage",
            "command": "python3 scripts/validate_passage_quality.py --fix",
            "description": "ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸å“è³ªã®ä¿®æ­£"
        })
    
    return fixes
```

**åŠ¹æœ**:
- âœ… å˜ç´”ãªå•é¡Œã‚’è‡ªå‹•ä¿®æ­£
- âœ… äººé–“ã¯è¤‡é›‘ãªå•é¡Œã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹

---

### 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã®è¿½åŠ 

**ç¾çŠ¶**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ã®ã¿
**å¿…è¦æ€§**: ãƒ“ãƒ«ãƒ‰æ™‚é–“ã€å®Ÿè¡Œé€Ÿåº¦ã®è¿½è·¡

```python
def check_performance_metrics(self):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯"""
    self.log("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒã‚§ãƒƒã‚¯", "INFO")
    
    # 1. ãƒ“ãƒ«ãƒ‰æ™‚é–“æ¸¬å®š
    try:
        start_time = datetime.now()
        
        result = subprocess.run(
            ["npm", "run", "build"],
            cwd=self.base_dir,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        build_time = (datetime.now() - start_time).total_seconds()
        
        if build_time > 60:  # 60ç§’ä»¥ä¸Š
            self.add_issue(
                "performance",
                "WARNING",
                f"ãƒ“ãƒ«ãƒ‰æ™‚é–“ãŒé•·ã„: {build_time:.1f}ç§’ï¼ˆç›®æ¨™: <60ç§’ï¼‰",
                auto_fix=False
            )
        else:
            self.log(f"ãƒ“ãƒ«ãƒ‰æ™‚é–“: {build_time:.1f}ç§’", "SUCCESS")
            
    except subprocess.TimeoutExpired:
        self.add_issue(
            "performance",
            "CRITICAL",
            "ãƒ“ãƒ«ãƒ‰ãŒ5åˆ†ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“",
            auto_fix=False
        )
    
    # 2. dist/ ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    dist_path = self.base_dir / "dist"
    if dist_path.exists():
        total_size = sum(f.stat().st_size for f in dist_path.rglob('*') if f.is_file())
        size_mb = total_size / (1024 * 1024)
        
        if size_mb > 10:  # 10MBä»¥ä¸Š
            self.add_issue(
                "performance",
                "WARNING",
                f"ãƒ“ãƒ«ãƒ‰ã‚µã‚¤ã‚ºãŒå¤§ãã„: {size_mb:.1f}MBï¼ˆç›®æ¨™: <10MBï¼‰",
                auto_fix=False
            )
        else:
            self.log(f"ãƒ“ãƒ«ãƒ‰ã‚µã‚¤ã‚º: {size_mb:.1f}MB", "SUCCESS")
    
    # 3. lighthouse CI ã‚¹ã‚³ã‚¢ï¼ˆæ—¢å­˜ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒã‚ã‚Œã°ï¼‰
    lighthouse_report = self.base_dir / "lighthouse-report.json"
    if lighthouse_report.exists():
        with open(lighthouse_report, "r") as f:
            report = json.load(f)
            performance_score = report.get("categories", {}).get("performance", {}).get("score", 0) * 100
            
            if performance_score < 90:
                self.add_issue(
                    "performance",
                    "WARNING",
                    f"Lighthouseãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {performance_score}ï¼ˆç›®æ¨™: â‰¥90ï¼‰",
                    auto_fix=False
                )
```

**åŠ¹æœ**:
- âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚’æ—©æœŸæ¤œå‡º
- âœ… ãƒªãƒªãƒ¼ã‚¹å‰ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š

---

## ğŸ¯ å„ªå…ˆåº¦: ä¸­ï¼ˆ1-2é€±é–“ä»¥å†…ï¼‰

### 5. æ—¢å­˜ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨ã®é‡è¤‡æ’é™¤

**ç¾çŠ¶**: 20å€‹ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒå­˜åœ¨
```
auto-deploy.yml
auto-fix.yml
build.yml
grammar-quality-check.yml
health-check.yml
maintenance-ai.yml
quality-nervous-system.yml
smoke-test.yml
...
```

**å•é¡Œ**:
- ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
- å®Ÿè¡Œæ™‚é–“ã®ç„¡é§„
- CI/CD ã‚³ã‚¹ãƒˆã®å¢—åŠ 

#### æ”¹å–„æ¡ˆ: çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```yaml
# .github/workflows/integrated-quality-pipeline.yml
name: çµ±åˆå“è³ªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 18 * * *'  # æ¯æ—¥

jobs:
  # Job 1: é«˜é€Ÿãƒã‚§ãƒƒã‚¯ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
  fast-checks:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        check: [lint, typecheck, unit-test]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm ci
      - run: npm run ${{ matrix.check }}
  
  # Job 2: ãƒ‡ãƒ¼ã‚¿å“è³ªï¼ˆå“è³ªç¥çµŒç³»çµ±ï¼‰
  data-quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
      - run: python3 scripts/quality_nervous_system.py
  
  # Job 3: ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AIï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ™‚ã®ã¿ï¼‰
  maintenance:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    needs: [fast-checks, data-quality]
    steps:
      - uses: actions/checkout@v4
      - run: python3 scripts/maintenance_ai.py --verbose
  
  # Job 4: ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆmainãƒ–ãƒ©ãƒ³ãƒã®ã¿ï¼‰
  deploy:
    if: github.ref == 'refs/heads/main'
    needs: [fast-checks, data-quality]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: npm run build
      - run: npm run deploy
```

**åŠ¹æœ**:
- âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ•°ã‚’20å€‹ â†’ 5-7å€‹ã«å‰Šæ¸›
- âœ… CI/CD å®Ÿè¡Œæ™‚é–“ã‚’50%å‰Šæ¸›
- âœ… ä¿å®ˆæ€§å‘ä¸Š

---

### 6. é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®å¼·åŒ–

**ç¾çŠ¶**: GitHub Issueã®ã¿
**å¿…è¦æ€§**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥

#### A. Slackçµ±åˆï¼ˆæ—¢ã«æº–å‚™æ¸ˆã¿ï¼‰

```python
# scripts/maintenance_ai.py
def send_slack_notification(self, report: Dict[str, Any]):
    """Slacké€šçŸ¥ã‚’é€ä¿¡"""
    webhook_url = os.environ.get("SLACK_WEBHOOK_URL")
    
    if not webhook_url:
        self.log("Slack webhook URLæœªè¨­å®š", "INFO")
        return
    
    # CRITICALãŒã‚ã‚‹å ´åˆã®ã¿é€šçŸ¥
    if report["critical_issues"] == 0:
        return
    
    message = {
        "text": "ğŸš¨ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AI: CRITICALå•é¡Œã‚’æ¤œå‡º",
        "blocks": [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": "ğŸ¤– å®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AI ãƒ¬ãƒãƒ¼ãƒˆ"
                }
            },
            {
                "type": "section",
                "fields": [
                    {"type": "mrkdwn", "text": f"*ç·å•é¡Œæ•°:* {report['total_issues']}"},
                    {"type": "mrkdwn", "text": f"*CRITICAL:* {report['critical_issues']}"},
                ]
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": "\n".join([
                        f"â€¢ *{i['category']}*: {i['description']}"
                        for i in report['issues']
                        if i['severity'] == 'CRITICAL'
                    ][:5])  # æœ€åˆã®5ä»¶ã®ã¿
                }
            },
            {
                "type": "actions",
                "elements": [
                    {
                        "type": "button",
                        "text": {"type": "plain_text", "text": "è©³ç´°ã‚’è¦‹ã‚‹"},
                        "url": f"https://github.com/{os.environ.get('GITHUB_REPOSITORY')}/actions"
                    }
                ]
            }
        ]
    }
    
    try:
        import requests
        response = requests.post(webhook_url, json=message, timeout=10)
        response.raise_for_status()
        self.log("Slacké€šçŸ¥é€ä¿¡æˆåŠŸ", "SUCCESS")
    except Exception as e:
        self.log(f"Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}", "WARNING")
```

#### B. Emailé€šçŸ¥

```python
def send_email_notification(self, report: Dict[str, Any]):
    """Emailé€šçŸ¥ï¼ˆGitHub Actionsã®æ©Ÿèƒ½ã‚’åˆ©ç”¨ï¼‰"""
    # GitHub Actionsã®æ¨™æº–å‡ºåŠ›ã‚’åˆ©ç”¨
    if report["critical_issues"] > 0:
        print("::error::CRITICALå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        print(f"::set-output name=has_critical::{True}")
```

```yaml
# .github/workflows/maintenance-ai.yml ã«è¿½åŠ 
- name: Emailé€šçŸ¥
  if: steps.maintenance.outputs.has_critical == 'true'
  uses: dawidd6/action-send-mail@v3
  with:
    server_address: smtp.gmail.com
    server_port: 465
    username: ${{secrets.MAIL_USERNAME}}
    password: ${{secrets.MAIL_PASSWORD}}
    subject: ğŸš¨ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹AI: CRITICALå•é¡Œ
    body: file://maintenance_report.json
    to: ${{secrets.NOTIFICATION_EMAIL}}
```

---

### 7. ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆæ”¹å–„

**ç¾çŠ¶**: 1å›ã®å®Ÿè¡Œçµæœã®ã¿
**å¿…è¦æ€§**: æ™‚ç³»åˆ—ã§ã®å“è³ªæ¨ç§»è¿½è·¡

```python
def track_quality_trends(self):
    """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¿½è·¡"""
    history_file = self.base_dir / ".maintenance_history.json"
    
    # å±¥æ­´èª­ã¿è¾¼ã¿
    history = []
    if history_file.exists():
        with open(history_file, "r") as f:
            history = json.load(f)
    
    # ä»Šå›ã®çµæœã‚’è¿½åŠ 
    current_result = {
        "timestamp": datetime.now().isoformat(),
        "total_issues": len(self.issues),
        "critical_issues": len([i for i in self.issues if i["severity"] == "CRITICAL"]),
        "warning_issues": len([i for i in self.issues if i["severity"] == "WARNING"]),
        "test_coverage": self._get_test_coverage(),
        "build_time": self._get_last_build_time(),
    }
    
    history.append(current_result)
    
    # æœ€æ–°30æ—¥åˆ†ã®ã¿ä¿æŒ
    history = history[-30:]
    
    # ä¿å­˜
    with open(history_file, "w") as f:
        json.dump(history, f, indent=2)
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    if len(history) >= 7:
        # 7æ—¥å‰ã¨æ¯”è¼ƒ
        prev = history[-7]
        current = history[-1]
        
        issue_trend = current["total_issues"] - prev["total_issues"]
        
        if issue_trend > 5:
            self.add_issue(
                "trend",
                "WARNING",
                f"å•é¡Œæ•°ãŒ7æ—¥å‰ã¨æ¯”è¼ƒã—ã¦{issue_trend}ä»¶å¢—åŠ ã—ã¦ã„ã¾ã™",
                auto_fix=False
            )
        elif issue_trend < -5:
            self.log(f"å“è³ªæ”¹å–„: å•é¡Œæ•°ãŒ{-issue_trend}ä»¶æ¸›å°‘ã—ã¾ã—ãŸ", "SUCCESS")
```

**ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«åŒ–**:
```python
def generate_trend_chart(self):
    """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆï¼ˆmatplotlibä½¿ç”¨ï¼‰"""
    try:
        import matplotlib.pyplot as plt
        
        history_file = self.base_dir / ".maintenance_history.json"
        with open(history_file, "r") as f:
            history = json.load(f)
        
        dates = [datetime.fromisoformat(h["timestamp"]) for h in history]
        issues = [h["total_issues"] for h in history]
        
        plt.figure(figsize=(10, 6))
        plt.plot(dates, issues, marker='o')
        plt.title("å•é¡Œæ•°ã®æ¨ç§»ï¼ˆéå»30æ—¥ï¼‰")
        plt.xlabel("æ—¥ä»˜")
        plt.ylabel("å•é¡Œæ•°")
        plt.grid(True)
        plt.savefig(self.base_dir / "maintenance_trend.png")
        
        self.log("ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•ç”Ÿæˆ: maintenance_trend.png", "SUCCESS")
        
    except ImportError:
        self.log("matplotlibãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“", "INFO")
```

---

## ğŸ¯ å„ªå…ˆåº¦: ä½ï¼ˆå°†æ¥çš„ãªæ‹¡å¼µï¼‰

### 8. AIé§†å‹•ã®å•é¡Œè¨ºæ–­

```python
def ai_diagnosis(self, issue: Dict[str, Any]) -> str:
    """AIã«ã‚ˆã‚‹å•é¡Œè¨ºæ–­ã¨ä¿®æ­£ææ¡ˆ"""
    # GitHub Copilot / OpenAI API ã‚’åˆ©ç”¨
    prompt = f"""
    ä»¥ä¸‹ã®å•é¡Œã‚’åˆ†æã—ã€ä¿®æ­£æ–¹æ³•ã‚’ææ¡ˆã—ã¦ãã ã•ã„:
    
    ã‚«ãƒ†ã‚´ãƒª: {issue['category']}
    é‡è¦åº¦: {issue['severity']}
    èª¬æ˜: {issue['description']}
    ãƒ•ã‚¡ã‚¤ãƒ«: {issue.get('file_path', 'N/A')}
    """
    
    # APIå‘¼ã³å‡ºã—ï¼ˆå®Ÿè£…çœç•¥ï¼‰
    # suggestion = call_ai_api(prompt)
    
    return suggestion
```

### 9. ã‚«ã‚¹ã‚¿ãƒ ãƒ«ãƒ¼ãƒ«ãƒ»ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ 

```python
# scripts/maintenance_plugins/custom_check.py
class CustomCheck:
    def check(self, project_dir: Path) -> List[Dict]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè£…"""
        issues = []
        
        # ä¾‹: NEW HORIZONå›ºæœ‰ã®æ–‡æ³•ãƒã‚§ãƒƒã‚¯
        # ...
        
        return issues

# maintenance_ai.py
def load_custom_plugins(self):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã¿"""
    plugin_dir = self.base_dir / "scripts" / "maintenance_plugins"
    
    for plugin_file in plugin_dir.glob("*.py"):
        # å‹•çš„import
        module = importlib.import_module(f"maintenance_plugins.{plugin_file.stem}")
        plugin = module.CustomCheck()
        
        issues = plugin.check(self.base_dir)
        self.issues.extend(issues)
```

---

## ğŸ“Š å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### Week 1: é«˜å„ªå…ˆåº¦ï¼ˆã™ãã«åŠ¹æœï¼‰
- âœ… CodeQLçµ±åˆ (1æ—¥)
- âœ… å“è³ªç¥çµŒç³»çµ±ã®JSONå‡ºåŠ›çµ±åˆ (1æ—¥)
- âœ… è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ã®æ‹¡å¼µ (2æ—¥)
- âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š (1æ—¥)

### Week 2-3: ä¸­å„ªå…ˆåº¦ï¼ˆåŠ¹ç‡åŒ–ï¼‰
- âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ»é‡è¤‡æ’é™¤ (3æ—¥)
- âœ… Slacké€šçŸ¥å®Ÿè£… (1æ—¥)
- âœ… ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ (2æ—¥)

### Month 2+: ä½å„ªå…ˆåº¦ï¼ˆé«˜åº¦åŒ–ï¼‰
- AIè¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ 
- ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ 
- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰é–‹ç™º

---

## ğŸ’¡ ã™ãã«è©¦ã›ã‚‹ Quick Wins

### 1. CodeQLã®æœ‰åŠ¹åŒ–ï¼ˆ5åˆ†ï¼‰

```bash
# .github/workflows/codeql.yml ã‚’ä½œæˆ
gh workflow enable codeql.yml
```

### 2. Dependabot PRã®è‡ªå‹•ãƒãƒ¼ã‚¸è¨­å®šï¼ˆ10åˆ†ï¼‰

```yaml
# .github/workflows/auto-merge-dependabot.yml
name: Auto-merge Dependabot PRs

on: pull_request

jobs:
  auto-merge:
    runs-on: ubuntu-latest
    if: github.actor == 'dependabot[bot]'
    steps:
      - uses: actions/checkout@v4
      - name: Enable auto-merge
        run: gh pr merge --auto --squash "$PR_URL"
        env:
          PR_URL: ${{github.event.pull_request.html_url}}
          GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}}
```

### 3. ãƒ“ãƒ«ãƒ‰æ™‚é–“æ¸¬å®šã®è¿½åŠ ï¼ˆ15åˆ†ï¼‰

```python
# scripts/maintenance_ai.py ã® check_file_sizes() ã®å¾Œã«è¿½åŠ 
self.check_performance_metrics()
```

---

## ğŸ¯ æœ€å„ªå…ˆã§å®Ÿè£…ã™ã¹ã3ã¤

1. **CodeQLçµ±åˆ** - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å‘ä¸Šï¼ˆ15åˆ†ï¼‰
1. **å“è³ªç¥çµŒç³»çµ±ã®JSONçµ±åˆ** - è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆï¼ˆ30åˆ†ï¼‰
1. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š** - ãƒ“ãƒ«ãƒ‰æ™‚é–“è¿½è·¡ï¼ˆ30åˆ†ï¼‰

åˆè¨ˆå®Ÿè£…æ™‚é–“: **1æ™‚é–“15åˆ†**

---

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ

| æ”¹å–„é …ç›® | ç¾çŠ¶ | æ”¹å–„å¾Œ | åŠ¹æœ |
|---------|------|--------|------|
| **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£** | npm auditã®ã¿ | +CodeQL | è„†å¼±æ€§æ¤œå‡ºç‡ +80% |
| **å•é¡Œæ¤œå‡ºç²¾åº¦** | åŸºæœ¬ãƒã‚§ãƒƒã‚¯ | +è©³ç´°çµ±åˆ | èª¤æ¤œçŸ¥ -50% |
| **CI/CDæ™‚é–“** | 20ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ | çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | å®Ÿè¡Œæ™‚é–“ -50% |
| **é€šçŸ¥é€Ÿåº¦** | Issueä½œæˆã®ã¿ | +Slack | å¿œç­”æ™‚é–“ -90% |
| **å“è³ªå¯è¦–åŒ–** | å˜ç™ºãƒ¬ãƒãƒ¼ãƒˆ | +ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ | å•é¡Œæ—©æœŸç™ºè¦‹ +70% |

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. ã“ã®ææ¡ˆã‚’ç¢ºèª
1. å„ªå…ˆåº¦ã®é«˜ã„æ”¹å–„ã‹ã‚‰å®Ÿè£…
1. 1é€±é–“å¾Œã«åŠ¹æœã‚’æ¸¬å®š
1. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãèª¿æ•´

**è³ªå•ã‚„æ‡¸å¿µãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ã”ç›¸è«‡ãã ã•ã„ï¼**
