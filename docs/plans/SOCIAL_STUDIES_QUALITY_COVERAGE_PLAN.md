# 社会科データ品質改善・網羅性検証計画

**策定日**: 2026年1月2日  
**対象**: 社会科教材（地理・歴史・公民）CSV データ  
**前提**: [SOCIAL_STUDIES_EXPANSION_PLAN.md](SOCIAL_STUDIES_EXPANSION_PLAN.md) の拡充計画を補完

---

## 📋 エグゼクティブサマリー

### 現状認識

社会科データは「東京書籍『新編 新しい社会』準拠」を標榜し、約1,500語句を収録していますが、以下の課題により**「単元別に重要事項を網羅している」と証明できない状態**です：

1. **データ品質問題（Critical）**
   - 関連事項・詳細解説に数値の3桁区切りカンマ（`1,000万`等）が混入
   - CSVの区切りカンマと干渉し、列ズレが発生（関連分野が242種類のノイズ値を含む）
   - アプリ側のCSVローダも単純 `split(',')` で同様の問題を抱える

2. **網羅性検証体制の不在（High）**
   - 東京書籍の章・単元構成との対照表がない
   - 単元ごとの必須語句（教科書の太字）カバレッジが不明
   - 「準拠」を主張する根拠が薄弱

### 目標

1. **データ品質の修復**：列ズレを解消し、正しい分類軸で検索・出題可能な状態にする
2. **網羅性の可視化**：東京書籍の単元構成に対するカバレッジを数値で示せる状態にする
3. **継続的品質保証**：新規データ追加時に品質・網羅性を自動チェックできる体制を確立

### 想定工数

- **Phase 1**: CSV修復と検証強化 → **2-3日**
- **Phase 2**: 単元リスト整備とカバレッジ監査 → **5-7日**
- **Phase 3**: 継続的品質保証の自動化 → **2-3日**

**合計**: 9-13日（1人月未満）

---

## 🔍 現状分析

### データ規模（2026年1月時点）

| 項目 | 現状 | 計画 | 達成率 |
|------|------|------|--------|
| 総語句数 | 1,518 | 1,500 | 101% ✅ |
| 歴史分野 | 約600 | 600 | ~100% |
| 地理分野 | 約450 | 450 | ~100% |
| 公民分野 | 約450 | 450 | ~100% |

### データ品質問題の詳細

#### 1. 値内カンマによる列ズレ

**問題の具体例**:

```csv
# 正常な行（9列）
徳川家康,とくがわいえやす,江戸幕府を開いた人物,...,関ヶ原の戦い|大坂の陣|幕藩体制,歴史-近世,人物名,,2

# 異常な行（値内カンマで列ズレ）
アラブ首長国連邦,あらぶしゅちょうこくれんぽう,中東の産油国,...,ドバイ|人口1,000万|石油|連邦,,1|2|3
                                                           ↑ カンマが区切りと誤認される
```

**影響範囲**:
- 関連事項に `人口1,000万` `約1,300km` 等の3桁区切り数値を含む行が多数存在
- 関連分野が本来13種類 → 242種類（ノイズ値229種類）に増加
- 暗記タブ・三択タブのフィルタ機能が事実上破綻

**根本原因**:
- CSVフォーマットとして RFC 4180（引用符エスケープ）未対応
- データ作成時に値内カンマのチェックが不十分
- アプリ側のCSVローダが単純 `split(',')` で実装

#### 2. 関連分野の不正値

**監査結果**（2026年1月2日実施）:

```
unique relatedFields: 242種類
  - 正常値: 13種類（歴史-古代、地理-日本 等）
  - ノイズ値: 229種類（人口1、3、ヨーロッパ、オタワ 等）
```

**ノイズ値の主な発生源**:
- 関連事項の一部が関連分野列に漏出（列ズレの結果）
- grade列の値（`1`, `2`, `3`）が関連分野として誤認識

### 網羅性検証の課題

#### 東京書籍の単元構成（参考：令和3年度版）

**歴史的分野**（例）:
- 第1章: 歴史の流れをつかもう
  - 1節: 文明のおこりと日本の成り立ち（縄文・弥生・古墳時代）
  - 2節: 古代国家の歩みと東アジア世界（飛鳥・奈良・平安時代）
- 第2章: 中世の日本
  - 1節: 武士の台頭と鎌倉幕府
  - 2節: 室町幕府と民衆の成長
- ...

**現状の課題**:
- 上記の単元構成に対する語句の配置が不明
- 各単元の「必須語句（教科書の太字）」との照合が未実施
- 「第2章1節が90%カバー、第3章2節が60%カバー」のような定量情報がない

---

## 🎯 改訂計画

### Phase 1: データ品質修復と検証強化（2-3日）

#### 目的
CSV列ズレを解消し、正しい分類軸で運用可能な状態にする

#### タスク

##### 1.1 値内カンマの一括除去

**対象ファイル**:
```
public/data/social-studies/all-social-studies.csv
public/data/social-studies/social-studies-history-40.csv
public/data/social-studies/social-studies-geography-30.csv
public/data/social-studies/social-studies-civics-30.csv
public/data/social-studies/social-studies-sample.csv
```

**修復方針**:
- 数値の3桁区切りカンマを除去（`1,000` → `1000`、`1,300km` → `1300km`）
- 単位表記は維持（`万人`、`km` 等）
- バックアップを作成してから一括修正

**ツール**: `scripts/fix-social-studies-csv-commas.ts`（既存、拡張必要）

**検証**:
```bash
# 修復前後の列数チェック
python3 -c "import csv; ..."

# 関連分野の種類数チェック（13種類になることを確認）
npx tsx scripts/audit-social-studies-coverage.ts --input public/data/social-studies/all-social-studies.csv
```

##### 1.2 アプリ側CSVローダの強化

**対象ファイル**: `src/utils/socialStudiesLoader.ts`

**修正内容**:
- RFC 4180準拠のCSVパーサに変更（引用符エスケープ対応）
- または、読み込み時に値内カンマを除去する前処理を追加
- 列数チェックを追加（9列固定、不足時はエラー）

**検証**:
```bash
npm run dev
# 暗記タブ・三択タブで社会科を選択し、フィルタが正常動作することを確認
```

##### 1.3 品質検証スクリプトの拡充

**既存**: `scripts/validate-social-studies.py` / `.ts`（旧形式前提で動作不全）

**新規・拡張**:
- `scripts/audit-social-studies-coverage.ts`（今回追加、値内カンマ対応を強化）
- 自動チェック項目:
  - 列数整合性（全行が9列）
  - 関連分野の正当性（13種類のみ許可）
  - grade値の妥当性（1, 2, 3, 1|2, 1|2|3 のみ許可）
  - 必須列の空欄チェック（語句、読み、詳細解説、関連分野）

**CI統合**:
```yaml
# .github/workflows/data-quality.yml（新規）
name: Social Studies Data Quality
on: [push, pull_request]
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: npx tsx scripts/audit-social-studies-coverage.ts --input public/data/social-studies/all-social-studies.csv
```

#### 成果物

- [ ] 全CSV修復済み（列ズレ解消、関連分野13種類）
- [ ] アプリ側ローダ修正済み（動作確認完了）
- [ ] 品質検証スクリプト拡充（自動チェック可能）
- [ ] CI統合（PR時に自動チェック）

---

### Phase 2: 単元リスト整備とカバレッジ監査（5-7日）

#### 目的
東京書籍の単元構成に対するカバレッジを数値化し、「準拠」の根拠を明確化

#### タスク

##### 2.1 東京書籍 単元リストの作成

**対象教科書**:
- 『新編 新しい社会 地理』（地理的分野）
- 『新編 新しい社会 歴史』（歴史的分野）
- 『新編 新しい社会 公民』（公民的分野）

**作成物**: `docs/references/tokyo-shoseki-units.json`

**構造**:
```json
{
  "units": [
    {
      "id": "history-ancient-1",
      "grade": "1",
      "field": "歴史-古代",
      "chapter": "第1章",
      "section": "1節",
      "label": "文明のおこりと日本の成り立ち",
      "requiredTerms": [
        "縄文時代", "弥生時代", "邪馬台国", "卑弥呼",
        "古墳時代", "前方後円墳", "大和朝廷"
      ],
      "importantTerms": [
        "竪穴住居", "高床倉庫", "魏志倭人伝"
      ]
    },
    {
      "id": "history-ancient-2",
      "grade": "1",
      "field": "歴史-古代",
      "chapter": "第1章",
      "section": "2節",
      "label": "古代国家の歩みと東アジア世界",
      "requiredTerms": [
        "聖徳太子", "冠位十二階", "十七条の憲法",
        "大化の改新", "奈良時代", "平安時代"
      ],
      "importantTerms": [
        "遣隋使", "遣唐使", "平城京", "平安京"
      ]
    }
  ]
}
```

**作成方法**:
1. 東京書籍の公式サイト（年間指導計画・内容解説資料）から単元構成を取得
2. 教科書の太字語句を `requiredTerms` に列挙（必須）
3. 太字以外の重要語句を `importantTerms` に列挙（推奨）
4. 各単元に20-30語句程度を配置

**想定規模**:
- 歴史: 20-25単元
- 地理: 15-20単元
- 公民: 15-20単元
- 合計: 50-65単元

##### 2.2 単元別カバレッジの算出

**ツール**: `scripts/audit-social-studies-coverage.ts`（既存、`--units` オプション使用）

**実行例**:
```bash
npx tsx scripts/audit-social-studies-coverage.ts \
  --input public/data/social-studies/all-social-studies.csv \
  --units docs/references/tokyo-shoseki-units.json
```

**出力イメージ**:
```
=== 単元別カバレッジ ===

[OK] 歴史-古代 第1章1節: 文明のおこりと日本の成り立ち (history-ancient-1)
  required: 7, present: 7, missing: 0, coverage: 100%

[WARN] 歴史-古代 第1章2節: 古代国家の歩みと東アジア世界 (history-ancient-2)
  required: 6, present: 5, missing: 1, coverage: 83%
  missingTerms: 平安京

[NG] 歴史-中世 第2章1節: 武士の台頭と鎌倉幕府 (history-medieval-1)
  required: 8, present: 5, missing: 3, coverage: 62%
  missingTerms: 執権政治 / 御成敗式目 / 元寇（2回目）

--- 総合カバレッジ ---
required: 450, present: 398, coverage: 88%
```

##### 2.3 不足語句の補完計画

**カバレッジ基準**:
- 100%: 追加不要
- 80-99%: 優先度High（次回追加候補）
- 60-79%: 優先度Medium
- 0-59%: 優先度Critical（早急に補完）

**作業内容**:
1. カバレッジ60%未満の単元をリストアップ
2. 不足語句を `local-data-packs/social-studies-補完-YYYYMMDD.csv` として作成
3. 品質ガイドに従って語句データを作成（正確性重視）
4. 既存CSVにマージ
5. カバレッジ再測定

##### 2.4 CSVへの単元IDの付与（オプション）

**目的**: 各語句がどの単元に属するかを明示

**修正内容**:
- CSVに `unitIds` 列を追加（例: `history-ancient-1|history-ancient-2`）
- 単元IDは複数可（複数単元で扱われる語句に対応）

**メリット**:
- 単元別の出題が可能になる
- 進捗管理が単元単位で可能
- 学習順序の最適化に活用

**デメリット**:
- 既存データへの付与作業が必要（半自動化可能）

**決定**: Phase 2完了後、効果を評価してから判断

#### 成果物

- [ ] 東京書籍 単元リスト完成（50-65単元、必須語句列挙）
- [ ] 単元別カバレッジレポート（数値で網羅性を証明）
- [ ] 不足語句の補完計画（優先度付き）
- [ ] （オプション）CSVへの単元ID付与

---

### Phase 3: 継続的品質保証の自動化（2-3日）

#### 目的
新規データ追加時に品質・網羅性を自動チェックし、劣化を防止

#### タスク

##### 3.1 pre-commit フックの追加

**対象**: 社会科CSVファイルの変更時

**チェック内容**:
```bash
#!/bin/bash
# .husky/pre-commit（既存に追記）

echo "🔍 社会科CSVの品質チェック..."

# 列数・関連分野の整合性チェック
npx tsx scripts/audit-social-studies-coverage.ts \
  --input public/data/social-studies/all-social-studies.csv \
  --strict

# 値内カンマの検出
if grep -E '[0-9],[0-9]' public/data/social-studies/*.csv; then
  echo "❌ 値内カンマが検出されました。修復してください。"
  exit 1
fi

echo "✅ 品質チェック完了"
```

##### 3.2 カバレッジの定期監視

**頻度**: 月次（データ追加のたびに自動実行）

**レポート保存先**: `docs/reports/coverage-YYYYMM.md`

**内容**:
- 単元別カバレッジの推移（80%未満の単元を重点監視）
- 新規追加語句の品質スコア（`validate-social-studies.py` の結果）
- 関連分野の分布（ノイズ値の混入検知）

##### 3.3 品質ダッシュボード（オプション）

**ツール**: GitHub Pages + JSON統計

**表示内容**:
- 総語句数の推移
- 単元別カバレッジのヒートマップ
- 品質スコアの推移（100点満点）

**実装**: Phase 3完了後、効果を評価してから判断

#### 成果物

- [ ] pre-commit フック追加（品質チェック自動化）
- [ ] 月次カバレッジレポートの自動生成
- [ ] （オプション）品質ダッシュボード

---

## 📊 成功基準

### Phase 1完了時（必須）

- [ ] 全CSV修復済み（列ズレ解消）
- [ ] 関連分野が13種類に収束
- [ ] アプリのフィルタ機能が正常動作
- [ ] CI/CD統合完了

### Phase 2完了時（必須）

- [ ] 単元リスト完成（50-65単元）
- [ ] 総合カバレッジ 80%以上
- [ ] カバレッジ60%未満の単元が10%以下

### Phase 3完了時（推奨）

- [ ] pre-commit フック動作確認
- [ ] 月次レポート自動生成

### 最終目標（Phase 2-3完了後）

**「東京書籍『新編 新しい社会』準拠」を数値で証明できる状態**

- 総合カバレッジ: **85%以上**
- 単元別カバレッジ: **80%以上が全体の90%**
- データ品質スコア: **平均90点以上**
- 継続的監視: **月次レポート自動生成**

---

## ⚠️ リスクと対策

### リスク1: 東京書籍の単元リスト作成の工数超過

**原因**: 教科書の実物確認や公式資料の入手に時間がかかる

**対策**:
- 年間指導計画（公式PDF）を第一優先で活用
- 帝国書院など他社の教科書も参考にして単元構造を類推
- 必須語句は最初は少なめ（単元あたり10-15語）から開始し、段階的に拡充

### リスク2: カバレッジが予想より低い

**原因**: 既存データが特定単元に偏っている

**対策**:
- Phase 2.3（不足語句の補完）を優先実施
- 外部AI（ChatGPT等）を活用して不足語句のドラフトを生成
- 品質ガイドに従った人間によるレビューを必須化

### リスク3: 値内カンマの除去でデータの意味が変わる

**原因**: `1,000万人` → `1000万人` で可読性が低下

**対策**:
- 修復前にバックアップを必ず作成
- 修復前後のdiffを目視確認
- ユーザーテストで可読性を評価（必要に応じて元に戻す or 別表記を検討）

---

## 📅 実施スケジュール（案）

| Phase | タスク | 想定日数 | 担当 | 開始予定 |
|-------|--------|----------|------|----------|
| Phase 1 | 1.1 値内カンマ除去 | 0.5日 | AI | Day 1 |
| Phase 1 | 1.2 ローダ強化 | 1日 | AI | Day 1-2 |
| Phase 1 | 1.3 検証スクリプト拡充 | 0.5-1日 | AI | Day 2-3 |
| Phase 2 | 2.1 単元リスト作成 | 3-4日 | Human + AI | Day 4-7 |
| Phase 2 | 2.2 カバレッジ算出 | 0.5日 | AI | Day 8 |
| Phase 2 | 2.3 不足語句補完 | 1-2日 | Human + AI | Day 9-10 |
| Phase 3 | 3.1 pre-commit追加 | 0.5日 | AI | Day 11 |
| Phase 3 | 3.2 定期監視設定 | 0.5日 | AI | Day 11 |
| 検証 | 統合テスト | 1日 | All | Day 12-13 |

**合計**: 9-13日

---

## 🔄 次のアクション

### 即座に実施（Phase 1開始）

1. ✅ 改訂計画の承認を得る（本ドキュメント）
2. [ ] Phase 1.1: 値内カンマ除去スクリプトの実行
3. [ ] Phase 1.2: アプリ側ローダの修正
4. [ ] Phase 1.3: 品質検証スクリプトの拡充

### Phase 1完了後

5. [ ] Phase 2.1: 東京書籍 単元リストの作成開始
6. [ ] 中間レビュー（カバレッジ速報、方針調整）

---

## 📚 関連ドキュメント

- [SOCIAL_STUDIES_EXPANSION_PLAN.md](SOCIAL_STUDIES_EXPANSION_PLAN.md) - 拡充計画（語句数目標）
- [social-studies-quality-enforcement.instructions.md](../../.aitk/instructions/social-studies-quality-enforcement.instructions.md) - 品質ガイド
- [scripts/audit-social-studies-coverage.ts](../../scripts/audit-social-studies-coverage.ts) - カバレッジ監査ツール
- [scripts/fix-social-studies-csv-commas.ts](../../scripts/fix-social-studies-csv-commas.ts) - CSV修復ツール

---

**承認者**: （ユーザー名）  
**承認日**: YYYY年MM月DD日  
**ステータス**: 🟡 承認待ち
