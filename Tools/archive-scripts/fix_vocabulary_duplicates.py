#!/usr/bin/env python3
"""
èªå½™CSVé‡è¤‡ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python3 scripts/fix_vocabulary_duplicates.py
    python3 scripts/fix_vocabulary_duplicates.py --file junior-high-entrance-words.csv
    python3 scripts/fix_vocabulary_duplicates.py --dry-run
"""

import csv
import sys
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple
import argparse

VOCAB_DIR = Path("nanashi8.github.io/public/data/vocabulary")

def find_duplicates(file_path: Path) -> Dict[str, List[int]]:
    """CSVå†…ã®èªå¥é‡è¤‡ã‚’æ¤œå‡º"""
    duplicates = defaultdict(list)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader, start=1):
            word = row['èªå¥'].strip().lower()
            duplicates[word].append(idx)
    
    # é‡è¤‡ã®ã¿æŠ½å‡º
    return {word: rows for word, rows in duplicates.items() if len(rows) > 1}

def remove_duplicates(file_path: Path, dry_run: bool = False) -> Tuple[int, int]:
    """é‡è¤‡ã‚’å‰Šé™¤ã—ã¦ä¸Šæ›¸ãä¿å­˜"""
    seen = set()
    unique_rows = []
    duplicate_count = 0
    
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames
        
        for row in reader:
            word = row['èªå¥'].strip().lower()
            if word in seen:
                duplicate_count += 1
                continue
            seen.add(word)
            unique_rows.append(row)
    
    if not dry_run:
        with open(file_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(unique_rows)
    
    return len(unique_rows), duplicate_count

def main():
    parser = argparse.ArgumentParser(description='èªå½™CSVã®é‡è¤‡ä¿®æ­£')
    parser.add_argument('--file', help='ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç† (ä¾‹: junior-high-entrance-words.csv)')
    parser.add_argument('--dry-run', action='store_true', help='å®Ÿéš›ã«ã¯ä¿®æ­£ã›ãšã€ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿')
    args = parser.parse_args()
    
    if args.file:
        files = [VOCAB_DIR / args.file]
    else:
        files = [
            VOCAB_DIR / "junior-high-entrance-words.csv",
            VOCAB_DIR / "intermediate-1800-words.csv"
        ]
    
    print("ğŸ”§ èªå½™é‡è¤‡ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    total_removed = 0
    
    for file_path in files:
        if not file_path.exists():
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
        
        print(f"\nğŸ“„ å‡¦ç†ä¸­: {file_path.name}")
        
        # é‡è¤‡æ¤œå‡º
        duplicates = find_duplicates(file_path)
        if not duplicates:
            print(f"  âœ… é‡è¤‡ãªã—")
            continue
        
        print(f"  âš ï¸ é‡è¤‡æ¤œå‡º: {len(duplicates)}èª")
        
        # æœ€åˆã®3ä»¶ã‚’è¡¨ç¤º
        for i, (word, rows) in enumerate(list(duplicates.items())[:3]):
            print(f"    - '{word}': è¡Œ{', '.join(map(str, rows))}")
        
        if len(duplicates) > 3:
            print(f"    ... ä»– {len(duplicates) - 3} ä»¶")
        
        # é‡è¤‡å‰Šé™¤
        unique_count, removed = remove_duplicates(file_path, args.dry_run)
        total_removed += removed
        
        if args.dry_run:
            print(f"  ğŸ” [dry-run] å‰Šé™¤äºˆå®š: {removed}ä»¶ (æ®‹ã‚Š: {unique_count}ä»¶)")
        else:
            print(f"  âœ… å‰Šé™¤å®Œäº†: {removed}ä»¶ (æ®‹ã‚Š: {unique_count}ä»¶)")
    
    print("\n" + "=" * 60)
    if args.dry_run:
        print(f"ğŸ” [dry-run] å‰Šé™¤äºˆå®šåˆè¨ˆ: {total_removed}ä»¶")
        print("\nå®Ÿéš›ã«ä¿®æ­£ã™ã‚‹å ´åˆã¯ --dry-run ã‚’å¤–ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        print(f"âœ… ä¿®æ­£å®Œäº†: {total_removed}ä»¶ã®é‡è¤‡ã‚’å‰Šé™¤")
        print("\nå†æ¤œè¨¼ã—ã¦ãã ã•ã„:")
        print("  python3 scripts/validate_all_content.py")
    
    return 0 if total_removed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())
