#!/usr/bin/env python3
"""
ç¬¬3æ®µéš: æ–‡è„ˆã‹ã‚‰ã®æ„å‘³æ¨è«–ï¼ˆæ”¹è‰¯ç‰ˆï¼‰

ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®æ—¥æœ¬èªè¨³ã‹ã‚‰å˜èªã®æ„å‘³ã‚’æ¨è«–
- ã‚«ã‚¿ã‚«ãƒŠéƒ¨åˆ†ã®æ¤œå‡º
- å“è©ãƒãƒ¼ã‚«ãƒ¼ã®åˆ©ç”¨ï¼ˆã€Œã€œã™ã‚‹ã€ã€Œã€œãªã€ãªã©ï¼‰
- å‰å¾Œã®è¨³èªã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
"""

import json
from pathlib import Path
from collections import defaultdict
import re


def extract_katakana(text):
    """ã‚«ã‚¿ã‚«ãƒŠéƒ¨åˆ†ã‚’æŠ½å‡º"""
    # ã‚«ã‚¿ã‚«ãƒŠãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºï¼ˆè¨˜å·å«ã‚€ï¼‰
    katakana_pattern = r'[ã‚¡-ãƒ´ãƒ¼ãƒ»]+'
    matches = re.findall(katakana_pattern, text)
    # 1æ–‡å­—ã®ã‚«ã‚¿ã‚«ãƒŠã‚„è¨˜å·ã®ã¿ã¯é™¤å¤–
    return [m for m in matches if len(m) > 1 and not m.replace('ãƒ»', '').replace('ãƒ¼', '') == '']


def infer_meaning_from_japanese(word, japanese_translations):
    """æ—¥æœ¬èªè¨³ã‹ã‚‰æ„å‘³ã‚’æ¨è«–"""
    # è¤‡æ•°ã®è¨³ã‹ã‚‰å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹
    all_katakana = []
    all_translations = []
    
    for jp in japanese_translations[:5]:  # æœ€åˆã®5ã¤ã®ä¾‹ã‚’ä½¿ç”¨
        # ã‚«ã‚¿ã‚«ãƒŠã‚’æŠ½å‡º
        katakana = extract_katakana(jp)
        all_katakana.extend(katakana)
        all_translations.append(jp)
    
    # ã‚«ã‚¿ã‚«ãƒŠãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
    if all_katakana:
        # æœ€ã‚‚é »å‡ºã™ã‚‹ã‚«ã‚¿ã‚«ãƒŠ
        katakana_freq = defaultdict(int)
        for k in all_katakana:
            katakana_freq[k] += 1
        
        most_common = max(katakana_freq.items(), key=lambda x: x[1])
        if most_common[1] >= 2:  # 2å›ä»¥ä¸Šå‡ºç¾
            return most_common[0]
    
    # ã‚«ã‚¿ã‚«ãƒŠãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æ—¥æœ¬èªè¨³ã‹ã‚‰æ¨æ¸¬
    # å˜èªã®é•·ã•ã«åŸºã¥ã„ã¦è¨³ã®å€™è£œã‚’çµã‚Šè¾¼ã‚€
    word_lower = word.lower()
    
    # çŸ­ã„å˜èªã¯åŸºæœ¬çš„ãªæ„å‘³ã®å¯èƒ½æ€§ãŒé«˜ã„
    if len(word_lower) <= 4:
        # é »å‡ºã™ã‚‹çŸ­ã„å˜èªã®å€™è£œ
        common_short_words = {
            'met': 'ä¼šã£ãŸ',
            'ate': 'é£Ÿã¹ãŸ',
            'grew': 'æˆé•·ã—ãŸãƒ»è‚²ã£ãŸ',
            'held': 'é–‹å‚¬ã—ãŸãƒ»æŒã£ãŸ',
            'wore': 'ç€ã¦ã„ãŸ',
            'born': 'ç”Ÿã¾ã‚ŒãŸ',
            'kept': 'ä¿ã£ãŸãƒ»å®ˆã£ãŸ',
            'lent': 'è²¸ã—ãŸ',
            'won': 'å‹ã£ãŸ',
            'lost': 'å¤±ã£ãŸ',
            'felt': 'æ„Ÿã˜ãŸ',
            'left': 'å»ã£ãŸãƒ»æ®‹ã—ãŸ',
            'sent': 'é€ã£ãŸ',
            'came': 'æ¥ãŸ',
            'went': 'è¡Œã£ãŸ',
            'gave': 'ä¸ãˆãŸ',
            'took': 'å–ã£ãŸ',
            'made': 'ä½œã£ãŸ',
            'said': 'è¨€ã£ãŸ',
            'told': 'è©±ã—ãŸ',
            'knew': 'çŸ¥ã£ã¦ã„ãŸ',
            'got': 'å¾—ãŸ',
            'saw': 'è¦‹ãŸ',
            'put': 'ç½®ã„ãŸ',
            'ran': 'èµ°ã£ãŸ',
            'sat': 'åº§ã£ãŸ',
            'cut': 'åˆ‡ã£ãŸ',
            'let': 'ã•ã›ãŸ',
            'set': 'è¨­å®šã—ãŸ',
            'hit': 'æ‰“ã£ãŸ',
            'shut': 'é–‰ã‚ãŸ',
            'hurt': 'å‚·ã¤ã‘ãŸ',
            'cost': 'è²»ç”¨ãŒã‹ã‹ã£ãŸ',
            'cast': 'æŠ•ã’ãŸ',
            'beat': 'æ‰“ã¡è² ã‹ã—ãŸ',
            'quit': 'ã‚„ã‚ãŸ',
            'read': 'èª­ã‚“ã ',
            'lead': 'å°ã„ãŸ',
            'feed': 'é£Ÿã¹ç‰©ã‚’ä¸ãˆãŸ',
            'paid': 'æ”¯æ‰•ã£ãŸ',
            'laid': 'ç½®ã„ãŸ',
            'sold': 'å£²ã£ãŸ',
            'told': 'è©±ã—ãŸ',
            'hung': 'ã‹ã‘ãŸ',
            'shot': 'æ’ƒã£ãŸ',
            'won\'t': 'ã—ãªã„ã ã‚ã†',
            'can\'t': 'ã§ããªã„',
            'don\'t': 'ã—ãªã„',
            'didn\'t': 'ã—ãªã‹ã£ãŸ',
            'isn\'t': 'ã§ã¯ãªã„',
            'wasn\'t': 'ã§ã¯ãªã‹ã£ãŸ',
            'aren\'t': 'ã§ã¯ãªã„',
            'weren\'t': 'ã§ã¯ãªã‹ã£ãŸ',
        }
        
        if word_lower in common_short_words:
            return common_short_words[word_lower]
    
    # æ—¥æœ¬èªè¨³ã‹ã‚‰å€™è£œã‚’æŠ½å‡º
    # ã€Œã®ã€ã‚„ã€Œã‚’ã€ã§åˆ†å‰²ã—ã¦åè©å¥ã‚’æ¢ã™
    candidates = []
    for jp in all_translations[:3]:
        # ã€Œã®ã€ã€Œã‚’ã€ã€Œã«ã€ã€ŒãŒã€ã€Œã¯ã€ã§åˆ†å‰²
        parts = re.split(r'[ã®ã‚’ã«ãŒã¯]', jp)
        for part in parts:
            part = part.strip()
            # ã‚«ã‚¿ã‚«ãƒŠä»¥å¤–ã§ã€é©åº¦ãªé•·ã•ã®å€™è£œ
            if part and not re.match(r'^[ã‚¡-ãƒ´ãƒ¼ãƒ»]+$', part) and 1 < len(part) < 10:
                candidates.append(part)
    
    if candidates:
        # æœ€ã‚‚é »å‡ºã™ã‚‹å€™è£œ
        cand_freq = defaultdict(int)
        for c in candidates:
            cand_freq[c] += 1
        
        most_common_cand = max(cand_freq.items(), key=lambda x: x[1])
        if most_common_cand[1] >= 2:
            return most_common_cand[0]
    
    return None


def main():
    # ãƒ‘ã‚¹ã®è¨­å®š
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    passages_dir = Path('public/data/passages-phrase-learning')
    
    # è¾æ›¸ã‚’èª­ã¿è¾¼ã¿
    print("ğŸ“– è¾æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    with open(dict_path, 'r', encoding='utf-8') as f:
        dictionary = json.load(f)
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰(è¦ç¢ºèª)å˜èªã¨æ–‡è„ˆã‚’åé›†
    print("ğŸ“Š (è¦ç¢ºèª)å˜èªã¨æ–‡è„ˆã‚’åé›†ä¸­...")
    word_contexts = defaultdict(lambda: {'translations': [], 'count': 0})
    
    for passage_file in passages_dir.glob('*.json'):
        with open(passage_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for phrase in data.get('phrases', []):
            japanese = phrase.get('japanese', '')
            for segment in phrase.get('segments', []):
                word = segment.get('word', '')
                meaning = segment.get('meaning', '')
                
                if meaning == '(è¦ç¢ºèª)' and word:
                    word_contexts[word]['translations'].append(japanese)
                    word_contexts[word]['count'] += 1
    
    print(f"  è¦‹ã¤ã‹ã£ãŸ(è¦ç¢ºèª)å˜èª: {len(word_contexts)}ç¨®é¡")
    
    # æ„å‘³ã‚’æ¨è«–
    print("\nğŸ” æ–‡è„ˆã‹ã‚‰æ„å‘³ã‚’æ¨è«–ä¸­...")
    inferred_words = {}
    
    for word, context in word_contexts.items():
        meaning = infer_meaning_from_japanese(word, context['translations'])
        if meaning:
            inferred_words[word.lower()] = {
                'meaning': meaning,
                'count': context['count'],
                'confidence': 'medium'
            }
    
    print(f"  æ¨è«–å¯èƒ½ãªå˜èª: {len(inferred_words)}èª")
    print(f"  ç·å‡ºç¾å›æ•°: {sum(w['count'] for w in inferred_words.values())}å›")
    
    if not inferred_words:
        print("\næ¨è«–ã§ãã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # è¾æ›¸ã‚’æ›´æ–°
    print("\nğŸ“ è¾æ›¸ã‚’æ›´æ–°ä¸­...")
    updated_count = 0
    for word_lower, info in inferred_words.items():
        if word_lower in dictionary:
            if dictionary[word_lower].get('meaning') == '(è¦ç¢ºèª)':
                dictionary[word_lower]['meaning'] = info['meaning']
                dictionary[word_lower]['source'] = 'context-inference'
                updated_count += 1
    
    # è¾æ›¸ã‚’ä¿å­˜
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(dictionary, f, ensure_ascii=False, indent=2)
    
    print(f"  è¾æ›¸æ›´æ–°: {updated_count}èª")
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    print("\nğŸ“ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ä¸­...")
    total_updated = 0
    
    for passage_file in sorted(passages_dir.glob('*.json')):
        with open(passage_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        file_updated = 0
        for phrase in data.get('phrases', []):
            for segment in phrase.get('segments', []):
                word = segment.get('word', '')
                word_lower = word.lower()
                
                if word_lower in inferred_words and segment.get('meaning') == '(è¦ç¢ºèª)':
                    segment['meaning'] = inferred_words[word_lower]['meaning']
                    file_updated += 1
        
        if file_updated > 0:
            with open(passage_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"  {passage_file.name}: {file_updated}ç®‡æ‰€æ›´æ–°")
            total_updated += file_updated
    
    print(f"\nâœ… å®Œäº†!")
    print(f"  æ¨è«–ã—ãŸå˜èª: {len(inferred_words)}èª")
    print(f"  æ›´æ–°ã—ãŸç®‡æ‰€: {total_updated}ç®‡æ‰€")
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    print("\nğŸ“Š æ¨è«–ä¾‹ï¼ˆå‡ºç¾é »åº¦ä¸Šä½20ï¼‰:")
    sorted_inferred = sorted(inferred_words.items(), key=lambda x: x[1]['count'], reverse=True)
    for i, (word, info) in enumerate(sorted_inferred[:20], 1):
        print(f"  {i}. {word}: {info['meaning']} ({info['count']}å›)")
    
    # æ®‹ã‚Šã®çµ±è¨ˆ
    print("\nğŸ“Š æ®‹ã‚Šã®(è¦ç¢ºèª)å˜èª:")
    remaining_unconfirmed = sum(1 for v in dictionary.values() if v.get('meaning') == '(è¦ç¢ºèª)')
    total_words = len(dictionary)
    has_meaning = sum(1 for v in dictionary.values() if v.get('meaning', '') and v.get('meaning', '') not in ['(è¦ç¢ºèª)', '(æœªç™»éŒ²)', '(å›ºæœ‰åè©)'])
    print(f"  å®Œæˆç‡: {has_meaning}/{total_words} ({has_meaning/total_words*100:.1f}%)")
    print(f"  æ®‹ã‚Š(è¦ç¢ºèª): {remaining_unconfirmed}èª")


if __name__ == '__main__':
    main()
