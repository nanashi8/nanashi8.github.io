#!/usr/bin/env python3
"""
ä½é »åº¦å˜èª(3å›å‡ºç¾)ã‚’ä¿®æ­£

æ‰‹å‹•ã§ä½œæˆã—ãŸè¾æ›¸ã‚’ä½¿ç”¨ã—ã¦ã€3å›å‡ºç¾ã®(è¦ç¢ºèª)å˜èªã‚’ä¿®æ­£ã—ã¾ã™ã€‚
"""

import json
from pathlib import Path

# 3å›å‡ºç¾å˜èªã®è¾æ›¸
LOW_FREQUENCY_3X_DICT = {
    'accessibility': 'ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚„ã™ã•ãƒ»åˆ©ç”¨å¯èƒ½æ€§',
    'acoustic': 'éŸ³éŸ¿ã®',
    'ailments': 'ç—…æ°—ãƒ»ä¸èª¿',
    'architectural': 'å»ºç¯‰ã®',
    'arrangements': 'é…ç½®ãƒ»æ‰‹é…',
    'assassination': 'æš—æ®º',
    'biologist': 'ç”Ÿç‰©å­¦è€…',
    'broader': 'ã‚ˆã‚Šåºƒã„',
    'cards': 'ã‚«ãƒ¼ãƒ‰',
    'cellular': 'ç´°èƒã®',
    'cleaner': 'ã‚ˆã‚Šæ¸…æ½”ãªãƒ»æ¸…æƒå“¡',
    'computational': 'è¨ˆç®—ã®',
    'correlates': 'ç›¸é–¢ã™ã‚‹',
    'counselor': 'ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼',
    'credit': 'ä¿¡ç”¨ãƒ»ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ',
    'culinary': 'æ–™ç†ã®',
    'curricula': 'ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ',
    'curtains': 'ã‚«ãƒ¼ãƒ†ãƒ³',
    'discount': 'å‰²å¼•',
    'displaced': 'ç§»å‹•ã•ã›ã‚‰ã‚ŒãŸ',
    'distribute': 'é…å¸ƒã™ã‚‹',
    'documentaries': 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼',
    'domains': 'é ˜åŸŸãƒ»åˆ†é‡',
    'downtown': 'ç¹è¯è¡—ãƒ»ä¸­å¿ƒè¡—',
    'drought': 'å¹²ã°ã¤',
    'economical': 'çµŒæ¸ˆçš„ãª',
    'elaborate': 'è©³ã—ãèª¬æ˜ã™ã‚‹ãƒ»ç²¾å·§ãª',
    'encounter': 'é­é‡ã™ã‚‹',
    "era's": 'æ™‚ä»£ã®',
    'existential': 'å®Ÿå­˜çš„ãª',
    'expansion': 'æ‹¡å¤§',
    'extract': 'æŠ½å‡ºã™ã‚‹',
    'fertile': 'è‚¥æ²ƒãª',
    'flavors': 'å‘³ãƒ»é¢¨å‘³',
    'fog': 'éœ§',
    'forgotten': 'å¿˜ã‚Œã‚‰ã‚ŒãŸ',
    'generational': 'ä¸–ä»£ã®',
    'grab': 'ã¤ã‹ã‚€',
    "grandmother's": 'ç¥–æ¯ã®',
    'handmade': 'æ‰‹ä½œã‚Šã®',
    'hockey': 'ãƒ›ãƒƒã‚±ãƒ¼',
    "hospital's": 'ç—…é™¢ã®',
    'ideal': 'ç†æƒ³çš„ãª',
    'ignore': 'ç„¡è¦–ã™ã‚‹',
    'implications': 'å½±éŸ¿ãƒ»æ„å‘³',
    'institutions': 'æ©Ÿé–¢ãƒ»åˆ¶åº¦',
    'interpretations': 'è§£é‡ˆ',
    'locate': 'ä½ç½®ã‚’ç‰¹å®šã™ã‚‹',
    'logistics': 'ç‰©æµãƒ»å¾Œæ–¹æ”¯æ´',
    'longevity': 'é•·å¯¿',
    'miniaturization': 'å°å‹åŒ–',
    'mortality': 'æ­»äº¡ç‡',
    'necessities': 'å¿…éœ€å“',
    'nurture': 'è‚²ã¦ã‚‹',
    'oldest': 'æœ€ã‚‚å¤ã„',
    "one's": 'è‡ªåˆ†ã®',
    'pairs': 'å¯¾ãƒ»ãƒšã‚¢',
    'pose': 'å¼•ãèµ·ã“ã™ãƒ»ãƒãƒ¼ã‚ºã‚’ã¨ã‚‹',
    'possess': 'æ‰€æœ‰ã™ã‚‹',
    'precipitation': 'é™æ°´',
    'preventable': 'äºˆé˜²å¯èƒ½ãª',
    'procedure': 'æ‰‹é †ãƒ»å‡¦ç½®',
    'promoted': 'ä¿ƒé€²ã•ã‚ŒãŸ',
    'qubits': 'é‡å­ãƒ“ãƒƒãƒˆ',
    'radiation': 'æ”¾å°„ç·š',
    'ramen': 'ãƒ©ãƒ¼ãƒ¡ãƒ³',
    'reasonable': 'åˆç†çš„ãª',
    'refreshing': 'çˆ½ã‚„ã‹ãª',
    'refrigerator': 'å†·è”µåº«',
    'regarding': 'ã€œã«é–¢ã—ã¦',
    'regulate': 'è¦åˆ¶ã™ã‚‹',
    'requirements': 'å¿…è¦æ¡ä»¶',
    'resistance': 'æŠµæŠ—',
    'responsible': 'è²¬ä»»ãŒã‚ã‚‹',
    'reveal': 'æ˜ã‚‰ã‹ã«ã™ã‚‹',
    'reveals': 'æ˜ã‚‰ã‹ã«ã™ã‚‹',
    'rural': 'ç”°èˆã®',
    'screenings': 'æ¤œæŸ»ãƒ»ä¸Šæ˜ ',
    'segregation': 'åˆ†é›¢ãƒ»éš”é›¢',
    'session': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ»ä¼šåˆ',
    'shelves': 'æ£š',
    'shorter': 'ã‚ˆã‚ŠçŸ­ã„',
    'simpler': 'ã‚ˆã‚Šç°¡å˜ãª',
    'singers': 'æ­Œæ‰‹',
    'slavery': 'å¥´éš·åˆ¶',
    "society's": 'ç¤¾ä¼šã®',
    'strokes': 'è„³å’ä¸­',
    'tech': 'æŠ€è¡“',
    'terrestrial': 'åœ°çƒã®ãƒ»é™¸ä¸Šã®',
    'territory': 'é ˜åŸŸ',
    'topsoil': 'è¡¨åœŸ',
    'uncertainties': 'ä¸ç¢ºå®Ÿæ€§',
    'underserved': 'ã‚µãƒ¼ãƒ“ã‚¹ãŒä¸è¶³ã—ã¦ã„ã‚‹',
    'voyages': 'èˆªæµ·',
    'wartime': 'æˆ¦æ™‚ä¸­',
}

def update_dictionary():
    """è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    updated = 0
    added = 0
    
    for word, meaning in LOW_FREQUENCY_3X_DICT.items():
        if word in dictionary:
            if dictionary[word].get('meaning') == '(è¦ç¢ºèª)':
                dictionary[word]['meaning'] = meaning
                dictionary[word]['source'] = 'low-frequency-3x-manual'
                updated += 1
        else:
            dictionary[word] = {
                'meaning': meaning,
                'source': 'low-frequency-3x-manual'
            }
            added += 1
    
    # ä¿å­˜
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(dictionary, f, ensure_ascii=False, indent=2)
    
    print(f'ğŸ“š è¾æ›¸æ›´æ–°: {updated}èª, æ–°è¦è¿½åŠ : {added}èª')
    return updated + added

def update_passage_files():
    """å…¨ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    passages_dir = Path('public/data/passages-phrase-learning')
    total_updates = 0
    file_count = 0
    
    for passage_file in sorted(passages_dir.glob('*.json')):
        with open(passage_file, encoding='utf-8') as f:
            data = json.load(f)
        
        file_updates = 0
        for phrase in data.get('phrases', []):
            for segment in phrase.get('segments', []):
                word = segment.get('word', '')
                
                if segment.get('meaning') == '(è¦ç¢ºèª)' and word in LOW_FREQUENCY_3X_DICT:
                    segment['meaning'] = LOW_FREQUENCY_3X_DICT[word]
                    file_updates += 1
        
        if file_updates > 0:
            # ä¿å­˜
            with open(passage_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            total_updates += file_updates
            file_count += 1
            print(f'  âœ… {passage_file.name}: {file_updates}ç®‡æ‰€')
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°: {file_count}ãƒ•ã‚¡ã‚¤ãƒ«, {total_updates}ç®‡æ‰€')
    return total_updates

def analyze_remaining():
    """æ®‹ã‚Šã®(è¦ç¢ºèª)ã‚’åˆ†æ"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    total = len(dictionary)
    confirmed = sum(1 for word_entry in dictionary.values() if word_entry.get('meaning') != '(è¦ç¢ºèª)')
    unconfirmed = total - confirmed
    
    print(f'\nğŸ“Š è¾æ›¸ã®çŠ¶æ…‹:')
    print(f'  ç·å˜èªæ•°: {total}èª')
    print(f'  æ„å‘³ç¢ºå®š: {confirmed}èª ({confirmed/total*100:.1f}%)')
    print(f'  (è¦ç¢ºèª): {unconfirmed}èª ({unconfirmed/total*100:.1f}%)')
    
    # ç›®æ¨™é”æˆçŠ¶æ³
    target_95 = int(total * 0.95)
    shortage = target_95 - confirmed
    print(f'\nğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:')
    print(f'  ç›®æ¨™(95%): {target_95}èª')
    print(f'  ä¸è¶³: {shortage}èª')
    print(f'  é”æˆç‡: {confirmed/target_95*100:.1f}%')
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹
    passages_dir = Path('public/data/passages-phrase-learning')
    unconfirmed_count = 0
    
    for f in passages_dir.glob('*.json'):
        data = json.load(open(f, encoding='utf-8'))
        for p in data.get('phrases', []):
            for s in p.get('segments', []):
                if s.get('meaning', '') == '(è¦ç¢ºèª)':
                    unconfirmed_count += 1
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹:')
    print(f'  (è¦ç¢ºèª)ç®‡æ‰€: {unconfirmed_count}ç®‡æ‰€')

def main():
    print('=' * 60)
    print('ä½é »åº¦å˜èª(3å›å‡ºç¾)ã‚’ä¿®æ­£')
    print('=' * 60)
    print()
    
    print(f'å¯¾è±¡: {len(LOW_FREQUENCY_3X_DICT)}èª\n')
    
    # è¾æ›¸ã‚’æ›´æ–°
    dict_updates = update_dictionary()
    print()
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    passage_updates = update_passage_files()
    
    # çµæœåˆ†æ
    analyze_remaining()
    
    print()
    print('=' * 60)
    print(f'âœ… å®Œäº†: è¾æ›¸{dict_updates}èªã€ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸{passage_updates}ç®‡æ‰€ã‚’æ›´æ–°')
    print('=' * 60)

if __name__ == '__main__':
    main()
