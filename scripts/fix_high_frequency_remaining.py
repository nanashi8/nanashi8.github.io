#!/usr/bin/env python3
"""
æ®‹ã‚Šã®é«˜é »åº¦å˜èª(5å›ä»¥ä¸Š)ã‚’ä¿®æ­£

æ‰‹å‹•ã§ä½œæˆã—ãŸè¾æ›¸ã‚’ä½¿ç”¨ã—ã¦ã€é »å‡ºã™ã‚‹(è¦ç¢ºèª)å˜èªã‚’ä¿®æ­£ã—ã¾ã™ã€‚
"""

import json
from pathlib import Path

# é«˜é »åº¦å˜èªã®è¾æ›¸ï¼ˆ5å›ä»¥ä¸Šå‡ºç¾ï¼‰
HIGH_FREQUENCY_DICT = {
    # å‹•è©ã®éå»å½¢ãƒ»æ´¾ç”Ÿå½¢
    'wondered': 'ä¸æ€è­°ã«æ€ã£ãŸ',
    'warned': 'è­¦å‘Šã—ãŸ',
    'fascinated': 'é­…äº†ã•ã‚ŒãŸ',
    'frozen': 'å‡ã£ãŸãƒ»å‡çµã—ãŸ',
    'predict': 'äºˆæ¸¬ã™ã‚‹',
    'restore': 'å¾©å…ƒã™ã‚‹ãƒ»å›å¾©ã™ã‚‹',
    
    # åè©
    'rejection': 'æ‹’çµ¶ãƒ»å´ä¸‹',
    'calculations': 'è¨ˆç®—',
    'dinosaur': 'æç«œ',
    'observation': 'è¦³å¯Ÿ',
    'cricket': 'ã‚¯ãƒªã‚±ãƒƒãƒˆãƒ»ã‚³ã‚ªãƒ­ã‚®',
    'kids': 'å­ä¾›ãŸã¡',
    'attitudes': 'æ…‹åº¦ãƒ»å§¿å‹¢',
    'men': 'ç”·æ€§ãŸã¡',
    'amounts': 'é‡ãƒ»é‡‘é¡',
    'artifacts': 'éºç‰©ãƒ»å·¥èŠ¸å“',
    'expectations': 'æœŸå¾…',
    'circumstances': 'çŠ¶æ³ãƒ»äº‹æƒ…',
    'shift': 'å¤‰åŒ–ãƒ»ã‚·ãƒ•ãƒˆ',
    'expectancy': 'æœŸå¾…ãƒ»äºˆæƒ³',
    'layer': 'å±¤',
    
    # å½¢å®¹è©
    'electrical': 'é›»æ°—ã®',
    'immediate': 'å³åº§ã®ãƒ»ç›´æ¥ã®',
    'atmospheric': 'å¤§æ°—ã®ãƒ»é›°å›²æ°—ã®',
    'persistent': 'æŒç¶šçš„ãªãƒ»æ ¹æ°—å¼·ã„',
    'lower': 'ã‚ˆã‚Šä½ã„',
    
    # åè©ï¼ˆæŠ½è±¡ï¼‰
    'estimate': 'æ¨å®šãƒ»è¦‹ç©ã‚‚ã‚Š',
    'decline': 'æ¸›å°‘ãƒ»è¡°é€€',
    'warmth': 'æš–ã‹ã•',
    
    # ãã®ä»–
    'exceeding': 'è¶…ãˆã‚‹',
    
    # å›ºæœ‰åè©
    'Sachiko': 'å¹¸å­ï¼ˆäººåï¼‰',
}

def update_dictionary():
    """è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    updated = 0
    for word, meaning in HIGH_FREQUENCY_DICT.items():
        if word in dictionary and dictionary[word].get('meaning') == '(è¦ç¢ºèª)':
            dictionary[word]['meaning'] = meaning
            dictionary[word]['source'] = 'high-frequency-manual'
            updated += 1
    
    # ä¿å­˜
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(dictionary, f, ensure_ascii=False, indent=2)
    
    print(f'ğŸ“š è¾æ›¸æ›´æ–°: {updated}èª')
    return updated

def update_passage_files():
    """å…¨ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    passages_dir = Path('public/data/passages-phrase-learning')
    total_updates = 0
    file_count = 0
    
    for passage_file in sorted(passages_dir.glob('*.json')):
        with open(passage_file, encoding='utf-8') as f:
            data = json.load(f)
        
        file_updates = 0
        for phrase in data.get('phrases', []):
            for segment in phrase.get('segments', []):
                word = segment.get('word', '')
                
                if segment.get('meaning') == '(è¦ç¢ºèª)' and word in HIGH_FREQUENCY_DICT:
                    segment['meaning'] = HIGH_FREQUENCY_DICT[word]
                    file_updates += 1
        
        if file_updates > 0:
            # ä¿å­˜
            with open(passage_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            total_updates += file_updates
            file_count += 1
            print(f'  âœ… {passage_file.name}: {file_updates}ç®‡æ‰€')
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°: {file_count}ãƒ•ã‚¡ã‚¤ãƒ«, {total_updates}ç®‡æ‰€')
    return total_updates

def analyze_remaining():
    """æ®‹ã‚Šã®(è¦ç¢ºèª)ã‚’åˆ†æ"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    total = len(dictionary)
    confirmed = sum(1 for word_entry in dictionary.values() if word_entry.get('meaning') != '(è¦ç¢ºèª)')
    unconfirmed = total - confirmed
    
    print(f'\nğŸ“Š è¾æ›¸ã®çŠ¶æ…‹:')
    print(f'  ç·å˜èªæ•°: {total}èª')
    print(f'  æ„å‘³ç¢ºå®š: {confirmed}èª ({confirmed/total*100:.1f}%)')
    print(f'  (è¦ç¢ºèª): {unconfirmed}èª ({unconfirmed/total*100:.1f}%)')
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹
    passages_dir = Path('public/data/passages-phrase-learning')
    unconfirmed_count = 0
    
    for f in passages_dir.glob('*.json'):
        data = json.load(open(f, encoding='utf-8'))
        for p in data.get('phrases', []):
            for s in p.get('segments', []):
                if s.get('meaning', '') == '(è¦ç¢ºèª)':
                    unconfirmed_count += 1
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹:')
    print(f'  (è¦ç¢ºèª)ç®‡æ‰€: {unconfirmed_count}ç®‡æ‰€')

def main():
    print('=' * 60)
    print('é«˜é »åº¦å˜èª(5å›ä»¥ä¸Š)ã‚’ä¿®æ­£')
    print('=' * 60)
    print()
    
    print(f'å¯¾è±¡: {len(HIGH_FREQUENCY_DICT)}èª\n')
    
    # è¾æ›¸ã‚’æ›´æ–°
    dict_updates = update_dictionary()
    print()
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    passage_updates = update_passage_files()
    
    # çµæœåˆ†æ
    analyze_remaining()
    
    print()
    print('=' * 60)
    print(f'âœ… å®Œäº†: è¾æ›¸{dict_updates}èªã€ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸{passage_updates}ç®‡æ‰€ã‚’æ›´æ–°')
    print('=' * 60)

if __name__ == '__main__':
    main()
