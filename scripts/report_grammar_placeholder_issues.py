#!/usr/bin/env python3
"""
æ–‡æ³•ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å•é¡Œã‚’è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ

485ä»¶ã®æ–‡æ³•ç”¨èªãŒæ—¥æœ¬èªè¨³ã«æ®‹ã£ã¦ã„ã‚‹å•é¡Œã‚’åˆ†æã—ã€
å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ãªå•é¡Œãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹
"""

import json
import re
from pathlib import Path
from collections import defaultdict

def analyze_grammar_file(file_path: Path) -> dict:
    """æ–‡æ³•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å•é¡Œã‚’æ¤œå‡º"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    issues = {
        'file': file_path.name,
        'total_questions': len(data.get('questions', [])),
        'placeholder_japanese': [],
        'placeholder_sentences': [],
        'placeholder_choices': [],
        'missing_translations': []
    }
    
    # æ–‡æ³•ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³(æ•°å­—ã§çµ‚ã‚ã‚‹)
    terminology_pattern = re.compile(r'^[^ã€‚]*\d+ã€‚$')
    
    for idx, q in enumerate(data.get('questions', []), 1):
        q_id = q.get('id', f'unknown-{idx}')
        japanese = q.get('japanese', '')
        sentence = q.get('sentence', '')
        choices = q.get('choices', [])
        
        # æ—¥æœ¬èªè¨³ãŒæ–‡æ³•ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒ
        if terminology_pattern.match(japanese):
            issues['placeholder_japanese'].append({
                'id': q_id,
                'japanese': japanese,
                'sentence': sentence,
                'type': q.get('type', 'unknown')
            })
        
        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡
        if 'Example sentence' in sentence or '____ blank' in sentence:
            issues['placeholder_sentences'].append({
                'id': q_id,
                'sentence': sentence
            })
        
        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼é¸æŠè‚¢
        if any(c in ['choice1', 'choice2', 'choice3'] for c in choices):
            issues['placeholder_choices'].append({
                'id': q_id,
                'choices': choices
            })
    
    return issues

def generate_report(grammar_dir: Path, output_file: Path):
    """å…¨æ–‡æ³•ãƒ•ã‚¡ã‚¤ãƒ«ã®å•é¡Œãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    affected_files = [
        'grammar_grade2_unit2.json',
        'grammar_grade2_unit3.json',
        'grammar_grade2_unit4.json',
        'grammar_grade2_unit5.json',
        'grammar_grade2_unit6.json',
        'grammar_grade2_unit7.json',
        'grammar_grade2_unit8.json',
        'grammar_grade2_unit9.json',
        'grammar_grade3_unit7.json'
    ]
    
    all_issues = []
    summary = {
        'total_files': 0,
        'total_placeholder_japanese': 0,
        'total_placeholder_sentences': 0,
        'total_placeholder_choices': 0
    }
    
    for filename in affected_files:
        file_path = grammar_dir / filename
        if not file_path.exists():
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}")
            continue
        
        print(f"åˆ†æä¸­: {filename}")
        issues = analyze_grammar_file(file_path)
        
        if issues['placeholder_japanese']:
            all_issues.append(issues)
            summary['total_files'] += 1
            summary['total_placeholder_japanese'] += len(issues['placeholder_japanese'])
            summary['total_placeholder_sentences'] += len(issues['placeholder_sentences'])
            summary['total_placeholder_choices'] += len(issues['placeholder_choices'])
    
    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# æ–‡æ³•ãƒ‡ãƒ¼ã‚¿ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å•é¡Œ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {Path(__file__).stat().st_mtime}\n\n")
        
        f.write("## ã‚µãƒãƒªãƒ¼\n\n")
        f.write(f"- å½±éŸ¿ã‚’å—ã‘ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: **{summary['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«**\n")
        f.write(f"- æ–‡æ³•ç”¨èªãŒæ®‹ã£ã¦ã„ã‚‹å•é¡Œæ•°: **{summary['total_placeholder_japanese']}ä»¶**\n")
        f.write(f"- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡ãŒæ®‹ã£ã¦ã„ã‚‹å•é¡Œæ•°: **{summary['total_placeholder_sentences']}ä»¶**\n")
        f.write(f"- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼é¸æŠè‚¢ãŒæ®‹ã£ã¦ã„ã‚‹å•é¡Œæ•°: **{summary['total_placeholder_choices']}ä»¶**\n\n")
        
        f.write("## é‡è¦æ€§\n\n")
        f.write("ã“ã‚Œã‚‰ã®å•é¡Œã¯**å®Ÿéš›ã®å•é¡Œãƒ‡ãƒ¼ã‚¿ã§ã¯ãªãã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®ã¾ã¾**ã«ãªã£ã¦ã„ã¾ã™ã€‚\n")
        f.write("ç”Ÿå¾’ã«ã“ã®ã¾ã¾å‡ºé¡Œã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚**ã™ã¹ã¦æ‰‹å‹•ã§é©åˆ‡ãªæ—¥æœ¬èªè¨³ã¨è‹±æ–‡ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚**\n\n")
        
        f.write("---\n\n")
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°
        for issues in all_issues:
            f.write(f"## {issues['file']}\n\n")
            f.write(f"- ç·å•é¡Œæ•°: {issues['total_questions']}\n")
            f.write(f"- æ–‡æ³•ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³: {len(issues['placeholder_japanese'])}ä»¶\n")
            f.write(f"- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡: {len(issues['placeholder_sentences'])}ä»¶\n")
            f.write(f"- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼é¸æŠè‚¢: {len(issues['placeholder_choices'])}ä»¶\n\n")
            
            if issues['placeholder_japanese']:
                f.write("### æ–‡æ³•ç”¨èªãŒæ®‹ã£ã¦ã„ã‚‹å•é¡Œ\n\n")
                
                # ã‚¿ã‚¤ãƒ—åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                by_type = defaultdict(list)
                for item in issues['placeholder_japanese']:
                    by_type[item['type']].append(item)
                
                for q_type, items in sorted(by_type.items()):
                    f.write(f"#### Type: {q_type}\n\n")
                    f.write("| ID | æ—¥æœ¬èª(æ–‡æ³•ç”¨èª) | è‹±æ–‡ |\n")
                    f.write("|---|---|---|\n")
                    for item in items:
                        japanese = item['japanese'].replace('|', '\\|')
                        sentence = item['sentence'].replace('|', '\\|')
                        f.write(f"| `{item['id']}` | {japanese} | {sentence} |\n")
                    f.write("\n")
            
            f.write("---\n\n")
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
        f.write("## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n\n")
        f.write("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: æ‰‹å‹•ã§å„å•é¡Œã‚’ä¿®æ­£\n\n")
        f.write("å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦ã€ä»¥ä¸‹ã‚’ä¿®æ­£:\n")
        f.write("1. `japanese` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: æ–‡æ³•ç”¨èª â†’ å®Ÿéš›ã®æ—¥æœ¬èªè¨³\n")
        f.write("2. `sentence` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ â†’ å®Ÿéš›ã®è‹±æ–‡\n")
        f.write("3. `choices` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: choice1/2/3 â†’ å®Ÿéš›ã®é¸æŠè‚¢\n")
        f.write("4. `explanation` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: é©åˆ‡ãªæ–‡æ³•èª¬æ˜ã‚’è¿½åŠ \n\n")
        
        f.write("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: å•é¡Œã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–\n\n")
        f.write("ã“ã‚Œã‚‰ã®unitã‚’ä¸€æ™‚çš„ã«éè¡¨ç¤ºã«ã—ã¦ã€å¾Œã§é©åˆ‡ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ:\n")
        f.write("- Grade 2ã®å…¨unit (unit2-9)\n")
        f.write("- Grade 3ã®unit7ã®ä¸€éƒ¨\n\n")
        
        f.write("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³3: AIæ”¯æ´ã§ã®å•é¡Œç”Ÿæˆ\n\n")
        f.write("å„æ–‡æ³•ãƒã‚¤ãƒ³ãƒˆã«å¯¾ã—ã¦ã€AIã‚’ä½¿ã£ã¦é©åˆ‡ãªå•é¡Œã‚’ç”Ÿæˆã—ã€\n")
        f.write("äººé–“ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ä¿®æ­£ã—ã¦ã‹ã‚‰åæ˜ ã™ã‚‹ã€‚\n\n")
    
    print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_file}")
    print(f"\nğŸ“Š ã‚µãƒãƒªãƒ¼:")
    print(f"   - å½±éŸ¿ãƒ•ã‚¡ã‚¤ãƒ«: {summary['total_files']}ä»¶")
    print(f"   - æ–‡æ³•ç”¨èª: {summary['total_placeholder_japanese']}ä»¶")
    print(f"   - ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡: {summary['total_placeholder_sentences']}ä»¶")
    print(f"   - ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼é¸æŠè‚¢: {summary['total_placeholder_choices']}ä»¶")

if __name__ == '__main__':
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    grammar_dir = project_root / 'public' / 'data' / 'grammar'
    output_file = project_root / 'docs' / 'reports' / 'GRAMMAR_PLACEHOLDER_ISSUES.md'
    
    # reportsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    generate_report(grammar_dir, output_file)
