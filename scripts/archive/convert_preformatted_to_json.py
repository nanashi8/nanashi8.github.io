#!/usr/bin/env python3
"""
æ”¹è¡Œæ¸ˆã¿ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ã‚ºå­¦ç¿’ç”¨JSONã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨ä¾‹:
    python3 convert_preformatted_to_json.py \
        --passage public/data/passages/intermediate-exchange-student-australia.txt \
        --translation public/data/passages-translations/intermediate-exchange-student-australia-ja.txt \
        --dictionary public/data/dictionaries/reading-passages-dictionary.json \
        --output public/data/passages-phrase-learning/intermediate-exchange-student-australia.json \
        --level intermediate \
        --theme "æ–‡åŒ–äº¤æµãƒ»å­¦æ ¡ç”Ÿæ´»"
"""

import json
import re
import argparse
from pathlib import Path


def load_dictionary(filepath):
    """è¾æ›¸JSONèª­ã¿è¾¼ã¿"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def split_long_sentence(sentence):
    """20èªã‚’è¶…ãˆã‚‹æ–‡ã‚’æ¥ç¶šè©ãƒ»é–¢ä¿‚è©ãƒ»å‰ç½®è©å¥ã§åˆ†å‰²"""
    words = sentence.split()
    if len(words) <= 20:
        return [sentence]
    
    # åˆ†å‰²å€™è£œã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„ªå…ˆåº¦é †ï¼‰
    
    # 1. å¾“å±æ¥ç¶šè©ï¼ˆæœ€å„ªå…ˆï¼‰
    subordinating_conjunctions = [
        'when', 'because', 'although', 'while', 'since',
        'after', 'before', 'unless', 'until', 'if', 'though', 
        'whereas', 'whenever', 'wherever'
    ]
    
    # 2. é–¢ä¿‚è©
    relative_pronouns = ['which', 'who', 'that', 'where', 'whose', 'whom']
    
    # 3. å‰ç½®è©ï¼ˆå¥ã®é–‹å§‹ä½ç½®ã¨ã—ã¦ï¼‰
    prepositions = [
        'with', 'without', 'by', 'during', 'through', 'throughout',
        'among', 'between', 'within', 'beyond', 'despite', 'regarding',
        'concerning', 'including', 'excluding', 'except', 'besides'
    ]
    
    # å…¨å€™è£œãƒªã‚¹ãƒˆ
    all_split_points = subordinating_conjunctions + relative_pronouns + prepositions
    
    # åˆ†å‰²ã‚’è©¦ã¿ã‚‹
    for keyword in all_split_points:
        # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå˜èªå¢ƒç•Œã§ï¼‰
        pattern = r'\b' + keyword + r'\b'
        match = re.search(pattern, sentence, re.IGNORECASE)
        
        if match:
            split_pos = match.start()
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰ã§åˆ†å‰²ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å¾ŒåŠã«å«ã‚ã‚‹ï¼‰
            before = sentence[:split_pos].rstrip(' ,')
            after = sentence[split_pos:].strip()
            
            # åˆ†å‰²å¾Œã®å„éƒ¨åˆ†ã®èªæ•°ã‚’ãƒã‚§ãƒƒã‚¯
            before_words = len(before.split())
            after_words = len(after.split())
            
            # å‰åŠãŒ5èªä»¥ä¸Šã€å¾ŒåŠãŒ3èªä»¥ä¸Šãªã‚‰åˆ†å‰²
            if before_words >= 5 and after_words >= 3:
                # å‰åŠæœ«å°¾ã«ã‚«ãƒ³ãƒãŒãªã„å ´åˆã¯è¿½åŠ 
                if not before.endswith(','):
                    before += ','
                return [before, after]
    
    # åˆ†å‰²ã§ããªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    return [sentence]


def load_phrases_from_preformatted_file(filepath):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ–‡å˜ä½ã§åˆ†å‰²ã—ã€é•·æ–‡ã¯ç¯€ãƒ»å¥ã§åˆ†å‰²"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    phrases = []
    
    # æ®µè½ã”ã¨ã«å‡¦ç†
    paragraphs = content.split('\n\n')
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        
        if not paragraph:
            continue
        
        # è¦‹å‡ºã—ï¼ˆæœ«å°¾ãŒ â€” ã®è¡Œï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if paragraph.endswith('â€”'):
            continue
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if paragraph.endswith(':') and len(paragraph.split()) <= 5:
            continue
        
        # æ–‡ã”ã¨ã«åˆ†å‰²ï¼ˆ.!?ã§åŒºåˆ‡ã‚‹ï¼‰
        sentences = re.split(r'(?<=[.!?])\s+', paragraph)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # 20èªã‚’è¶…ãˆã‚‹æ–‡ã¯åˆ†å‰²
            word_count = len(sentence.split())
            if word_count > 20:
                split_phrases = split_long_sentence(sentence)
                phrases.extend(split_phrases)
            else:
                phrases.append(sentence)
    
    return phrases


def load_japanese_phrases(filepath):
    """å…¨è¨³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¥æœ¬èªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’èª­ã¿è¾¼ã¿ï¼ˆè‹±æ–‡ã¨åŒã˜åˆ†å‰²ãƒ«ãƒ¼ãƒ«ï¼‰"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    japanese_phrases = []
    
    # æ®µè½ã”ã¨ã«å‡¦ç†
    paragraphs = content.split('\n\n')
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        
        if not paragraph:
            continue
        
        # è¦‹å‡ºã—ï¼ˆæœ«å°¾ãŒ â€” ã®è¡Œï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if paragraph.endswith('â€”'):
            continue
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if paragraph.endswith(':') and len(paragraph) <= 30:
            continue
        
        # æ—¥æœ¬èªã®æ–‡åŒºåˆ‡ã‚Šï¼ˆã€‚ï¼ï¼Ÿï¼‰ã§åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', paragraph)
        
        # å¥èª­ç‚¹ã‚’å‰ã®æ–‡ã«çµåˆ
        combined_sentences = []
        i = 0
        while i < len(sentences):
            if sentences[i].strip():
                sentence = sentences[i]
                # æ¬¡ãŒå¥èª­ç‚¹ãªã‚‰çµåˆ
                if i + 1 < len(sentences) and sentences[i + 1] in 'ã€‚ï¼ï¼Ÿ':
                    sentence += sentences[i + 1]
                    i += 2
                else:
                    i += 1
                combined_sentences.append(sentence.strip())
            else:
                i += 1
        
        japanese_phrases.extend(combined_sentences)
    
    return japanese_phrases


def create_segments_from_phrase(phrase, dictionary):
    """ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å˜èªã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†è§£"""
    # å¥èª­ç‚¹ã‚’åˆ†é›¢ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    words = re.findall(r"\w+(?:'\w+)?|[.,!?;:â€”\"']", phrase)
    
    segments = []
    for word in words:
        # å¥èª­ç‚¹ã¯ãã®ã¾ã¾è¿½åŠ 
        if word in ".,!?;:â€”\"'":
            segments.append({
                "word": word,
                "meaning": "",
                "isUnknown": False
            })
            continue
        
        # å˜èªã®æ„å‘³ã‚’è¾æ›¸ã‹ã‚‰æ¤œç´¢
        clean_word = word.lower()
        meaning = dictionary.get(clean_word, "")
        
        segments.append({
            "word": word,
            "meaning": meaning,
            "isUnknown": False
        })
    
    return segments


def detect_grammar_point(phrase):
    """ãƒ•ãƒ¬ãƒ¼ã‚ºã‹ã‚‰æ–‡æ³•ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    grammar_points = []
    
    # æ¥ç¶šè©
    if re.search(r'\b(when|if|because|although|while|since)\b', phrase, re.I):
        conjunctions = re.findall(r'\b(when|if|because|although|while|since)\b', phrase, re.I)
        grammar_points.append(f"{'/'.join(set([c.capitalize() for c in conjunctions]))}ç¯€")
    
    # thatç¯€
    if re.search(r'\bthat\b', phrase, re.I):
        grammar_points.append("thatç¯€")
    
    # å—å‹•æ…‹
    if re.search(r'\b(is|are|was|were|been)\s+\w+ed\b', phrase, re.I):
        grammar_points.append("å—å‹•æ…‹")
    
    # ç¾åœ¨å®Œäº†
    if re.search(r'\b(have|has)\s+\w+ed\b', phrase, re.I):
        grammar_points.append("ç¾åœ¨å®Œäº†")
    
    return " / ".join(grammar_points) if grammar_points else None


def convert_preformatted_to_json(passage_file, translation_file, dictionary_file, 
                                  output_file, level, theme):
    """æ”¹è¡Œæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥JSONç”Ÿæˆ"""
    
    print(f"ğŸ“– èª­ã¿è¾¼ã¿ä¸­...")
    print(f"  è‹±æ–‡: {passage_file}")
    print(f"  å…¨è¨³: {translation_file}")
    print(f"  è¾æ›¸: {dictionary_file}")
    
    # èª­ã¿è¾¼ã¿
    english_phrases = load_phrases_from_preformatted_file(passage_file)
    dictionary = load_dictionary(dictionary_file)
    japanese_phrases = load_japanese_phrases(translation_file)
    
    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
    with open(passage_file, 'r', encoding='utf-8') as f:
        first_line = f.readline().strip()
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸IDæŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰
    passage_id = Path(passage_file).stem
    
    print(f"\nğŸ“Š ãƒ•ãƒ¬ãƒ¼ã‚ºæ•°:")
    print(f"  è‹±èª: {len(english_phrases)} ãƒ•ãƒ¬ãƒ¼ã‚º")
    print(f"  æ—¥æœ¬èª: {len(japanese_phrases)} ãƒ•ãƒ¬ãƒ¼ã‚º")
    
    if len(english_phrases) != len(japanese_phrases):
        print(f"\nâš ï¸  è­¦å‘Š: ãƒ•ãƒ¬ãƒ¼ã‚ºæ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“ï¼")
        print(f"  ä¸è¶³åˆ†ã¯æ‰‹å‹•ã§èª¿æ•´ãŒå¿…è¦ã§ã™")
    
    # ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿ä½œæˆ
    print(f"\nğŸ”§ ãƒ•ãƒ¬ãƒ¼ã‚ºJSONç”Ÿæˆä¸­...")
    phrases_data = []
    
    for i, (en, ja) in enumerate(zip(english_phrases, japanese_phrases), 1):
        segments = create_segments_from_phrase(en, dictionary)
        grammar_point = detect_grammar_point(en)
        
        phrase_obj = {
            "id": i,
            "english": en,
            "japanese": ja,
            "phraseMeaning": ja,
            "segments": segments
        }
        
        # æ–‡æ³•ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Œã°è¿½åŠ 
        if grammar_point:
            phrase_obj["grammarPoint"] = grammar_point
        
        phrases_data.append(phrase_obj)
        
        if i % 10 == 0:
            print(f"  {i}/{len(english_phrases)} ãƒ•ãƒ¬ãƒ¼ã‚ºå‡¦ç†å®Œäº†")
    
    # ç·å˜èªæ•°è¨ˆç®—ï¼ˆå¥èª­ç‚¹é™¤ãï¼‰
    total_words = sum(
        len([s for s in p["segments"] if s["word"] not in ".,!?;:â€”\"'"])
        for p in phrases_data
    )
    
    # JSONå‡ºåŠ›
    passage_data = {
        "id": passage_id,
        "title": first_line.replace('â€”', ' - '),
        "level": level,
        "theme": theme,
        "actualWordCount": total_words,
        "phrases": phrases_data
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(passage_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç”Ÿæˆå®Œäº†!")
    print(f"  å‡ºåŠ›: {output_file}")
    print(f"  ãƒ•ãƒ¬ãƒ¼ã‚ºæ•°: {len(phrases_data)}")
    print(f"  ç·å˜èªæ•°: {total_words}")
    
    # è¾æ›¸æœªç™»éŒ²å˜èªã®ç¢ºèª
    missing_words = set()
    for phrase in phrases_data:
        for segment in phrase["segments"]:
            word = segment["word"]
            if word not in ".,!?;:â€”\"'" and not segment["meaning"]:
                missing_words.add(word.lower())
    
    if missing_words:
        print(f"\nâš ï¸  è¾æ›¸æœªç™»éŒ²å˜èª: {len(missing_words)}èª")
        print(f"  {', '.join(sorted(list(missing_words))[:10])}...")
    
    return passage_data


def main():
    parser = argparse.ArgumentParser(
        description='æ”¹è¡Œæ¸ˆã¿ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ã‚ºå­¦ç¿’ç”¨JSONã‚’ç”Ÿæˆ'
    )
    parser.add_argument('--passage', required=True, help='è‹±æ–‡ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«(.txt)')
    parser.add_argument('--translation', required=True, help='å…¨è¨³ãƒ•ã‚¡ã‚¤ãƒ«(.txt)')
    parser.add_argument('--dictionary', required=True, help='å˜èªè¾æ›¸(.json)')
    parser.add_argument('--output', required=True, help='å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--level', required=True, choices=['beginner', 'intermediate', 'advanced'], 
                       help='é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«')
    parser.add_argument('--theme', default='', help='ãƒ†ãƒ¼ãƒï¼ˆä»»æ„ï¼‰')
    
    args = parser.parse_args()
    
    convert_preformatted_to_json(
        args.passage,
        args.translation,
        args.dictionary,
        args.output,
        args.level,
        args.theme
    )


if __name__ == '__main__':
    main()
