#!/usr/bin/env python3
"""
å›ºæœ‰åè©ã¨ä¸€èˆ¬å˜èªï¼ˆå¤§æ–‡å­—å§‹ã¾ã‚Šï¼‰ã‚’å‡¦ç†

å¤§æ–‡å­—ã§å§‹ã¾ã‚‹å˜èªã‚’é©åˆ‡ã«åˆ†é¡ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚
"""

import json
from pathlib import Path

# å›ºæœ‰åè©è¾æ›¸
PROPER_NOUNS_DICT = {
    # äººå
    "Mika's": 'ãƒŸã‚«ã®',
    'Luther': 'ãƒ«ã‚¿ãƒ¼ï¼ˆäººåï¼‰',
    'Anderson': 'ã‚¢ãƒ³ãƒ€ãƒ¼ã‚½ãƒ³ï¼ˆäººåï¼‰',
    'Patel': 'ãƒ‘ãƒ†ãƒ«ï¼ˆäººåï¼‰',
    'Nakamura': 'ä¸­æ‘ï¼ˆäººåï¼‰',
    "Church's": 'æ•™ä¼šã®',
    "Darwin's": 'ãƒ€ãƒ¼ã‚¦ã‚£ãƒ³ã®',
    "Gandhi's": 'ã‚¬ãƒ³ã‚¸ãƒ¼ã®',
    "Columbus's": 'ã‚³ãƒ­ãƒ³ãƒ–ã‚¹ã®',
    "Magellan's": 'ãƒã‚¼ãƒ©ãƒ³ã®',
    "Moore's": 'ãƒ ãƒ¼ã‚¢ã®',
    "Children's": 'å­ä¾›ã®',
    "Women's": 'å¥³æ€§ã®',
    
    # åœ°åãƒ»æ–½è¨­å
    'Louvre': 'ãƒ«ãƒ¼ãƒ´ãƒ«ç¾è¡“é¤¨',
    'Nazareth': 'ãƒŠã‚¶ãƒ¬',
    'Texas': 'ãƒ†ã‚­ã‚µã‚¹',
    'Amazon': 'ã‚¢ãƒã‚¾ãƒ³',
    'Sentosa': 'ã‚»ãƒ³ãƒˆãƒ¼ã‚µ',
    
    # çµ„ç¹”ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰å
    'BTS': 'BTSï¼ˆéŸ“å›½ã®éŸ³æ¥½ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰',
    'Blackpink': 'BLACKPINKï¼ˆéŸ“å›½ã®éŸ³æ¥½ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰',
    'Bollywood': 'ãƒœãƒªã‚¦ãƒƒãƒ‰ï¼ˆã‚¤ãƒ³ãƒ‰æ˜ ç”»ï¼‰',
    'WeChat': 'WeChatï¼ˆä¸­å›½ã®ã‚¢ãƒ—ãƒªï¼‰',
    'Alipay': 'Alipayï¼ˆä¸­å›½ã®æ±ºæ¸ˆã‚µãƒ¼ãƒ“ã‚¹ï¼‰',
    'IC': 'ICï¼ˆé›†ç©å›è·¯ï¼‰',
    
    # æ­´å²ãƒ»æ–‡åŒ–ç”¨èª
    'Enlightenment': 'å•“è’™æ™‚ä»£',
    'Tamil': 'ã‚¿ãƒŸãƒ«èª',
    'Arabian': 'ã‚¢ãƒ©ãƒ“ã‚¢ã®',
    
    # ãã®ä»–
    'PelÃ©': 'ãƒšãƒ¬ï¼ˆã‚µãƒƒã‚«ãƒ¼é¸æ‰‹ï¼‰',
    'God': 'ç¥',
    'Rainforests': 'ç†±å¸¯é›¨æ—',
    'Laboratory': 'å®Ÿé¨“å®¤',
    'Biological': 'ç”Ÿç‰©å­¦çš„ãª',
    'Documentaries': 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼',
    'Improper': 'ä¸é©åˆ‡ãª',
}

# æ–‡é ­ã®ä¸€èˆ¬å˜èªï¼ˆå°æ–‡å­—å½¢å¼ã‚‚ç¢ºèªï¼‰
SENTENCE_START_WORDS = {
    'Wow': 'ã‚ã‚',
    'Hiking': 'ãƒã‚¤ã‚­ãƒ³ã‚°',
    'Rural': 'ç”°èˆã®ãƒ»åœ°æ–¹ã®',
    'Senior': 'å¹´é•·ã®ãƒ»ä¸Šç´šã®',
    'Pancakes': 'ãƒ‘ãƒ³ã‚±ãƒ¼ã‚­',
    'Disagreement': 'æ„è¦‹ã®ç›¸é•',
    'Fairy': 'å¦–ç²¾',
    'AV': 'AVï¼ˆè¦–è´è¦šï¼‰',
    'Decoration': 'è£…é£¾',
    'Publicity': 'å®£ä¼ãƒ»åºƒå ±',
    'Comets': 'å½—æ˜Ÿ',
    'Milky': 'ä¹³ç™½è‰²ã®',
}

def update_dictionary():
    """è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    # å…¨ã¦ã®è¾æ›¸ã‚’ãƒãƒ¼ã‚¸
    all_dict = {**PROPER_NOUNS_DICT, **SENTENCE_START_WORDS}
    
    updated = 0
    added = 0
    
    for word, meaning in all_dict.items():
        if word in dictionary:
            # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã‚’æ›´æ–°
            if dictionary[word].get('meaning') == '(è¦ç¢ºèª)':
                dictionary[word]['meaning'] = meaning
                if word in PROPER_NOUNS_DICT:
                    dictionary[word]['source'] = 'proper-noun'
                else:
                    dictionary[word]['source'] = 'sentence-start-word'
                updated += 1
        else:
            # æ–°è¦ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ 
            dictionary[word] = {
                'meaning': meaning,
                'source': 'proper-noun' if word in PROPER_NOUNS_DICT else 'sentence-start-word'
            }
            added += 1
    
    # ä¿å­˜
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(dictionary, f, ensure_ascii=False, indent=2)
    
    print(f'ğŸ“š è¾æ›¸æ›´æ–°: {updated}èª, æ–°è¦è¿½åŠ : {added}èª')
    print(f'  å›ºæœ‰åè©: {len(PROPER_NOUNS_DICT)}èª')
    print(f'  æ–‡é ­å˜èª: {len(SENTENCE_START_WORDS)}èª')
    return updated + added

def update_passage_files():
    """å…¨ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    passages_dir = Path('public/data/passages-phrase-learning')
    
    # å…¨ã¦ã®è¾æ›¸ã‚’ãƒãƒ¼ã‚¸
    all_dict = {**PROPER_NOUNS_DICT, **SENTENCE_START_WORDS}
    
    total_updates = 0
    file_count = 0
    
    for passage_file in sorted(passages_dir.glob('*.json')):
        with open(passage_file, encoding='utf-8') as f:
            data = json.load(f)
        
        file_updates = 0
        for phrase in data.get('phrases', []):
            for segment in phrase.get('segments', []):
                word = segment.get('word', '')
                
                if segment.get('meaning') == '(è¦ç¢ºèª)' and word in all_dict:
                    segment['meaning'] = all_dict[word]
                    file_updates += 1
        
        if file_updates > 0:
            # ä¿å­˜
            with open(passage_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            total_updates += file_updates
            file_count += 1
            print(f'  âœ… {passage_file.name}: {file_updates}ç®‡æ‰€')
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°: {file_count}ãƒ•ã‚¡ã‚¤ãƒ«, {total_updates}ç®‡æ‰€')
    return total_updates

def analyze_remaining():
    """æ®‹ã‚Šã®(è¦ç¢ºèª)ã‚’åˆ†æ"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    total = len(dictionary)
    confirmed = sum(1 for word_entry in dictionary.values() if word_entry.get('meaning') != '(è¦ç¢ºèª)')
    unconfirmed = total - confirmed
    
    print(f'\nğŸ“Š è¾æ›¸ã®çŠ¶æ…‹:')
    print(f'  ç·å˜èªæ•°: {total}èª')
    print(f'  æ„å‘³ç¢ºå®š: {confirmed}èª ({confirmed/total*100:.1f}%)')
    print(f'  (è¦ç¢ºèª): {unconfirmed}èª ({unconfirmed/total*100:.1f}%)')
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹
    passages_dir = Path('public/data/passages-phrase-learning')
    unconfirmed_count = 0
    
    for f in passages_dir.glob('*.json'):
        data = json.load(open(f, encoding='utf-8'))
        for p in data.get('phrases', []):
            for s in p.get('segments', []):
                if s.get('meaning', '') == '(è¦ç¢ºèª)':
                    unconfirmed_count += 1
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹:')
    print(f'  (è¦ç¢ºèª)ç®‡æ‰€: {unconfirmed_count}ç®‡æ‰€')

def main():
    print('=' * 60)
    print('å›ºæœ‰åè©ã¨å¤§æ–‡å­—å§‹ã¾ã‚Šå˜èªã‚’å‡¦ç†')
    print('=' * 60)
    print()
    
    print(f'å¯¾è±¡: {len(PROPER_NOUNS_DICT) + len(SENTENCE_START_WORDS)}èª')
    print(f'  å›ºæœ‰åè©: {len(PROPER_NOUNS_DICT)}èª')
    print(f'  æ–‡é ­å˜èª: {len(SENTENCE_START_WORDS)}èª\n')
    
    # è¾æ›¸ã‚’æ›´æ–°
    dict_updates = update_dictionary()
    print()
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    passage_updates = update_passage_files()
    
    # çµæœåˆ†æ
    analyze_remaining()
    
    print()
    print('=' * 60)
    print(f'âœ… å®Œäº†: è¾æ›¸{dict_updates}èªã€ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸{passage_updates}ç®‡æ‰€ã‚’æ›´æ–°')
    print('=' * 60)

if __name__ == '__main__':
    main()
