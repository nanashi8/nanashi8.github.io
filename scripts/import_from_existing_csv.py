#!/usr/bin/env python3
"""
æ—¢å­˜CSVã‹ã‚‰(è¦ç¢ºèª)å˜èªã®æ„å‘³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

intermediate-1800-words.csv ã¨ junior-high-entrance-words.csv ã«
å­˜åœ¨ã™ã‚‹å˜èªã®æ„å‘³ã‚’ reading-passages-dictionary.json ã¨
ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã«åæ˜ ã—ã¾ã™ã€‚
"""

import json
import csv
from pathlib import Path
from collections import defaultdict

def load_csv_vocabulary():
    """æ—¢å­˜CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èªå½™ã‚’èª­ã¿è¾¼ã‚€"""
    vocab_files = [
        'public/data/vocabulary/intermediate-1800-words.csv',
        'public/data/vocabulary/junior-high-entrance-words.csv'
    ]
    
    csv_vocab = {}
    for csv_file in vocab_files:
        source = 'intermediate-csv' if 'intermediate' in csv_file else 'junior-csv'
        print(f'ğŸ“– èª­ã¿è¾¼ã¿ä¸­: {csv_file}')
        
        with open(csv_file, encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                word = row['èªå¥'].strip()
                meaning = row['æ„å‘³'].strip()
                
                if word and meaning:
                    # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«ä¿å­˜
                    key = word.lower()
                    if key not in csv_vocab:
                        csv_vocab[key] = {
                            'word': word,
                            'meaning': meaning,
                            'source': source
                        }
    
    print(f'âœ… CSVèªå½™: {len(csv_vocab)}èª\n')
    return csv_vocab

def update_dictionary(csv_vocab):
    """è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    updated = 0
    for word, word_entry in dictionary.items():
        key = word.lower()
        
        # (è¦ç¢ºèª)ã§ã€ã‹ã¤CSVã«å­˜åœ¨ã™ã‚‹å ´åˆ
        if word_entry.get('meaning') == '(è¦ç¢ºèª)' and key in csv_vocab:
            csv_entry = csv_vocab[key]
            word_entry['meaning'] = csv_entry['meaning']
            word_entry['source'] = csv_entry['source']
            updated += 1
    
    # ä¿å­˜
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(dictionary, f, ensure_ascii=False, indent=2)
    
    print(f'ğŸ“š è¾æ›¸æ›´æ–°: {updated}èª')
    return updated

def update_passage_files(csv_vocab):
    """å…¨ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    passages_dir = Path('public/data/passages-phrase-learning')
    total_updates = 0
    file_count = 0
    
    for passage_file in sorted(passages_dir.glob('*.json')):
        with open(passage_file, encoding='utf-8') as f:
            data = json.load(f)
        
        file_updates = 0
        for phrase in data.get('phrases', []):
            for segment in phrase.get('segments', []):
                word = segment.get('word', '')
                key = word.lower()
                
                # (è¦ç¢ºèª)ã§ã€ã‹ã¤CSVã«å­˜åœ¨ã™ã‚‹å ´åˆ
                if segment.get('meaning') == '(è¦ç¢ºèª)' and key in csv_vocab:
                    csv_entry = csv_vocab[key]
                    segment['meaning'] = csv_entry['meaning']
                    file_updates += 1
        
        if file_updates > 0:
            # ä¿å­˜
            with open(passage_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            total_updates += file_updates
            file_count += 1
            print(f'  âœ… {passage_file.name}: {file_updates}ç®‡æ‰€')
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°: {file_count}ãƒ•ã‚¡ã‚¤ãƒ«, {total_updates}ç®‡æ‰€')
    return total_updates

def analyze_remaining():
    """æ®‹ã‚Šã®(è¦ç¢ºèª)ã‚’åˆ†æ"""
    dict_path = Path('public/data/dictionaries/reading-passages-dictionary.json')
    
    with open(dict_path, encoding='utf-8') as f:
        dictionary = json.load(f)
    
    total = len(dictionary)
    confirmed = sum(1 for word_entry in dictionary.values() if word_entry.get('meaning') != '(è¦ç¢ºèª)')
    unconfirmed = total - confirmed
    
    print(f'\nğŸ“Š è¾æ›¸ã®çŠ¶æ…‹:')
    print(f'  ç·å˜èªæ•°: {total}èª')
    print(f'  æ„å‘³ç¢ºå®š: {confirmed}èª ({confirmed/total*100:.1f}%)')
    print(f'  (è¦ç¢ºèª): {unconfirmed}èª ({unconfirmed/total*100:.1f}%)')
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹
    passages_dir = Path('public/data/passages-phrase-learning')
    unconfirmed_count = 0
    
    for f in passages_dir.glob('*.json'):
        data = json.load(open(f, encoding='utf-8'))
        for p in data.get('phrases', []):
            for s in p.get('segments', []):
                if s.get('meaning', '') == '(è¦ç¢ºèª)':
                    unconfirmed_count += 1
    
    print(f'\nğŸ“„ ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®çŠ¶æ…‹:')
    print(f'  (è¦ç¢ºèª)ç®‡æ‰€: {unconfirmed_count}ç®‡æ‰€')

def main():
    print('=' * 60)
    print('æ—¢å­˜CSVã‹ã‚‰å˜èªæ„å‘³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ')
    print('=' * 60)
    print()
    
    # CSVã‹ã‚‰èªå½™ã‚’èª­ã¿è¾¼ã¿
    csv_vocab = load_csv_vocabulary()
    
    # è¾æ›¸ã‚’æ›´æ–°
    dict_updates = update_dictionary(csv_vocab)
    print()
    
    # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    passage_updates = update_passage_files(csv_vocab)
    
    # çµæœåˆ†æ
    analyze_remaining()
    
    print()
    print('=' * 60)
    print(f'âœ… å®Œäº†: è¾æ›¸{dict_updates}èªã€ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸{passage_updates}ç®‡æ‰€ã‚’æ›´æ–°')
    print('=' * 60)

if __name__ == '__main__':
    main()
