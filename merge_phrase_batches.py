#!/usr/bin/env python3
"""
æ—¢å­˜ã®ç†Ÿèªãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¦ junior-high-entrance-phrases.csv ã‚’ç”Ÿæˆ
"""

import csv
import sys
from pathlib import Path
from collections import OrderedDict

def merge_phrase_batches():
    """ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ"""
    script_dir = Path(__file__).parent
    data_dir = script_dir / 'public/data'
    
    # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
    batch_files = [
        'batch2-phrases-100-beginner.csv',
        'batch3-phrases-100-beginner.csv',
        'batch4-phrases-100-beginner-final.csv',
        'batch5-phrases-100-intermediate.csv',
        'batch6-phrases-100-intermediate.csv',
        'batch7-phrases-100-intermediate.csv',
        'batch8-phrases-100-intermediate.csv',
        'batch9-phrases-100-advanced.csv',
        'batch10-phrases-100-advanced.csv',
        'batch11-phrases-100-final.csv',
    ]
    
    print('ğŸ“š ç†Ÿèªãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­...\n')
    
    # ç†Ÿèªã‚’æ ¼ç´ï¼ˆé‡è¤‡é™¤å»ã®ãŸã‚ OrderedDict ã‚’ä½¿ç”¨ï¼‰
    phrases = OrderedDict()
    difficulty_stats = {'åˆç´š': 0, 'ä¸­ç´š': 0, 'ä¸Šç´š': 0}
    
    # å„ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    for batch_file in batch_files:
        file_path = data_dir / batch_file
        
        if not file_path.exists():
            print(f'âš ï¸  {batch_file}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰')
            continue
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                count = 0
                
                for row in reader:
                    phrase = row['èªå¥'].strip()
                    difficulty = row['é›£æ˜“åº¦'].strip()
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®å‡ºç¾ã‚’ä¿æŒï¼‰
                    if phrase not in phrases:
                        phrases[phrase] = {
                            'èªå¥': phrase,
                            'èª­ã¿': row['èª­ã¿'].strip(),
                            'æ„å‘³': row['æ„å‘³'].strip(),
                            'èªæºç­‰è§£èª¬': row['èªæºç­‰è§£èª¬'].strip(),
                            'é–¢é€£èª': row['é–¢é€£èª'].strip(),
                            'é–¢é€£åˆ†é‡': row['é–¢é€£åˆ†é‡'].strip(),
                            'é›£æ˜“åº¦': difficulty
                        }
                        
                        if difficulty in difficulty_stats:
                            difficulty_stats[difficulty] += 1
                        
                        count += 1
                
                print(f'âœ… {batch_file}: {count}ç†Ÿèª')
        
        except Exception as e:
            print(f'âŒ {batch_file}: ã‚¨ãƒ©ãƒ¼ - {e}')
            continue
    
    print(f'\nğŸ“Š çµ±åˆçµæœ: {len(phrases)}ç†Ÿèªï¼ˆé‡è¤‡é™¤å»æ¸ˆã¿ï¼‰\n')
    
    # é›£æ˜“åº¦åˆ¥ã«ã‚½ãƒ¼ãƒˆ
    difficulty_order = {'åˆç´š': 1, 'ä¸­ç´š': 2, 'ä¸Šç´š': 3}
    sorted_phrases = sorted(
        phrases.values(),
        key=lambda x: (
            difficulty_order.get(x['é›£æ˜“åº¦'], 99),
            x['èªå¥'].lower()
        )
    )
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    output_path = data_dir / 'junior-high-entrance-phrases.csv'
    
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        fieldnames = ['èªå¥', 'èª­ã¿', 'æ„å‘³', 'èªæºç­‰è§£èª¬', 'é–¢é€£èª', 'é–¢é€£åˆ†é‡', 'é›£æ˜“åº¦']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        
        writer.writeheader()
        writer.writerows(sorted_phrases)
    
    print('ğŸ“Š é›£æ˜“åº¦åˆ¥çµ±è¨ˆ:')
    for difficulty, count in sorted(difficulty_stats.items(), key=lambda x: difficulty_order.get(x[0], 99)):
        print(f'  {difficulty}: {count}ç†Ÿèª')
    
    print(f'\nğŸ’¾ ä¿å­˜å®Œäº†: {output_path}')
    print(f'ğŸ“ ç·ç†Ÿèªæ•°: {len(phrases)}ç†Ÿèª\n')
    
    return len(phrases)

if __name__ == '__main__':
    try:
        total = merge_phrase_batches()
        print('âœ¨ çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸï¼')
        sys.exit(0)
    except Exception as e:
        print(f'\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')
        import traceback
        traceback.print_exc()
        sys.exit(1)
